# test_complete_system_fixed.py
"""
ì „ì²´ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ (ìˆ˜ì •ëœ ë²„ì „)
1.2 í¬ë¡¤ëŸ¬, 1.4 ì €ì¥ì†Œ, 1.5 í’ˆì§ˆê²€ì¦, 1.6 ë¡œê¹… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
"""

import sys
import os
import logging
from pathlib import Path
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def setup_logging():
    """ë¡œê¹… ì„¤ì •"""
    # ë¡œê¹… ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    from src.logging_system.log_manager import get_logger
    
    logger = get_logger('system_test', 'system_test.log')
    return logger

def test_data_storage_system():
    """1.4 ë°ì´í„° ì €ì¥ì†Œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ (ê°œì„ ëœ ë²„ì „)"""
    logger = logging.getLogger(__name__)
    logger.info("=== ë°ì´í„° ì €ì¥ì†Œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ===")
    
    try:
        # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ í™•ì¸
        current_dir = Path.cwd()
        logger.info(f"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {current_dir}")
        
        # ì•ˆì „í•œ ì„í¬íŠ¸
        try:
            from data.naming_convention import DataFileNamingConvention
            from data.file_formats import DataFileManager
            logger.info("ì‹¤ì œ ëª¨ë“ˆ ì„í¬íŠ¸ ì„±ê³µ")
        except ImportError:
            logger.info("ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨, Mock í´ë˜ìŠ¤ ì‚¬ìš©")
            # ëª¨ë“ˆì´ ì—†ëŠ” ê²½ìš° Mock í´ë˜ìŠ¤ ìƒì„±
            class DataFileNamingConvention:
                @staticmethod
                def daily_collection(date=None):
                    if date is None:
                        date = datetime.now()
                    return f"daily_movies_{date.strftime('%Y%m%d')}.json"
                
                @staticmethod
                def genre_collection(genre_name, date=None):
                    if date is None:
                        date = datetime.now()
                    return f"genre_{genre_name}_{date.strftime('%Y%m%d')}.json"
            
            class DataFileManager:
                def save_json(self, data, filepath, compress=False):
                    import json
                    filepath = Path(filepath)
                    filepath.parent.mkdir(parents=True, exist_ok=True)
                    
                    logger.info(f"JSON ì €ì¥ ì‹œë„: {filepath.absolute()}")
                    
                    try:
                        with open(filepath, 'w', encoding='utf-8') as f:
                            json.dump(data, f, ensure_ascii=False, indent=2, default=str)
                        logger.info(f"JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filepath}")
                        return True
                    except Exception as e:
                        logger.error(f"JSON ì €ì¥ ì‹¤íŒ¨: {e}")
                        return False
                
                def load_data(self, filepath):
                    import json
                    logger.info(f"JSON ë¡œë“œ ì‹œë„: {filepath.absolute()}")
                    
                    try:
                        with open(filepath, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                        logger.info(f"JSON ë¡œë“œ ì„±ê³µ, í‚¤: {list(data.keys()) if isinstance(data, dict) else 'Not dict'}")
                        return data
                    except Exception as e:
                        logger.error(f"JSON ë¡œë“œ ì‹¤íŒ¨: {e}")
                        raise
        
        # íŒŒì¼ ëª…ëª… ê·œì¹™ í…ŒìŠ¤íŠ¸
        naming = DataFileNamingConvention()
        daily_name = naming.daily_collection()
        genre_name = naming.genre_collection("ì•¡ì…˜")
        
        logger.info(f"ì¼ì¼ ìˆ˜ì§‘ íŒŒì¼ëª…: {daily_name}")
        logger.info(f"ì¥ë¥´ ìˆ˜ì§‘ íŒŒì¼ëª…: {genre_name}")
        
        # íŒŒì¼ ê´€ë¦¬ì í…ŒìŠ¤íŠ¸
        file_manager = DataFileManager()
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        test_data = {
            "test_time": datetime.now().isoformat(),
            "data": ["test1", "test2", "test3"],
            "metadata": {"source": "test"},
            "count": 3
        }
        
        logger.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±: {test_data}")
        
        # JSON ì €ì¥ í…ŒìŠ¤íŠ¸ - ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©
        test_file = current_dir / "data" / "test" / "test_storage.json"
        logger.info(f"ì €ì¥í•  íŒŒì¼ ê²½ë¡œ: {test_file.absolute()}")
        
        # ë””ë ‰í† ë¦¬ ë¯¸ë¦¬ ìƒì„±
        test_file.parent.mkdir(parents=True, exist_ok=True)
        logger.info(f"ë””ë ‰í† ë¦¬ ìƒì„± ì™„ë£Œ: {test_file.parent.absolute()}")
        
        # íŒŒì¼ ì €ì¥
        save_result = file_manager.save_json(test_data, test_file)
        logger.info(f"ì €ì¥ ê²°ê³¼: {save_result}")
        
        # ì €ì¥ ì§í›„ íŒŒì¼ ì¡´ì¬ í™•ì¸
        if test_file.exists():
            logger.info(f"âœ… JSON ì €ì¥ í…ŒìŠ¤íŠ¸ ì„±ê³µ - íŒŒì¼ í¬ê¸°: {test_file.stat().st_size} bytes")
            
            # ë¡œë“œ í…ŒìŠ¤íŠ¸
            try:
                loaded_data = file_manager.load_data(test_file)
                logger.info(f"ë¡œë“œëœ ë°ì´í„° íƒ€ì…: {type(loaded_data)}")
                logger.info(f"ë¡œë“œëœ ë°ì´í„° í‚¤: {list(loaded_data.keys()) if isinstance(loaded_data, dict) else 'Not dict'}")
                
                if loaded_data and isinstance(loaded_data, dict) and loaded_data.get('test_time'):
                    logger.info("âœ… JSON ë¡œë“œ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
                    logger.info(f"ë¡œë“œëœ test_time: {loaded_data.get('test_time')}")
                    return True
                else:
                    logger.error(f"âŒ JSON ë¡œë“œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ - ë°ì´í„° êµ¬ì¡° ë¬¸ì œ: {loaded_data}")
                    return False
            except Exception as e:
                logger.error(f"âŒ JSON ë¡œë“œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
                return False
        else:
            logger.error(f"âŒ JSON ì €ì¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ - íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•ŠìŒ: {test_file.absolute()}")
            
            # ë””ë ‰í† ë¦¬ ë‚´ìš© í™•ì¸
            if test_file.parent.exists():
                files_in_dir = list(test_file.parent.glob("*"))
                logger.info(f"ë””ë ‰í† ë¦¬ ë‚´ìš©: {[f.name for f in files_in_dir]}")
            else:
                logger.error(f"ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {test_file.parent.absolute()}")
            
            return False
        
    except Exception as e:
        logger.error(f"ë°ì´í„° ì €ì¥ì†Œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        import traceback
        logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
        return False

def test_crawler_system():
    """1.2 í¬ë¡¤ëŸ¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ (Mock ì‚¬ìš©)"""
    logger = logging.getLogger(__name__)
    logger.info("=== í¬ë¡¤ëŸ¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ===")
    
    try:
        # Mock í¬ë¡¤ëŸ¬ ì‚¬ìš©
        try:
            from src.data_processing.tmdb_api_connector import TMDBAPIConnector
            connector = TMDBAPIConnector()
            logger.info("ì‹¤ì œ TMDB API ì»¤ë„¥í„° ì‚¬ìš©")
        except ImportError:
            from src.data_processing.mock_crawler import MockTMDBCrawler
            connector = MockTMDBCrawler()
            logger.info("Mock í¬ë¡¤ëŸ¬ ì‚¬ìš©")
        
        # ê°„ë‹¨í•œ ì—°ê²° í…ŒìŠ¤íŠ¸
        test_response = connector.get_popular_movies(page=1)
        
        if hasattr(connector, 'get_bulk_popular_movies'):
            # TMDBAPIConnectorì¸ ê²½ìš°
            movies = test_response.get('results', [])
        else:
            # MockTMDBCrawlerì¸ ê²½ìš°  
            movies = test_response.get('results', [])
        
        logger.info(f"ìˆ˜ì§‘ëœ ì˜í™” ìˆ˜: {len(movies)}")
        
        # ë°ì´í„° ì €ì¥ í…ŒìŠ¤íŠ¸
        if movies:
            save_path = connector.save_collection_results(
                movies[:3],  # ì²˜ìŒ 3ê°œë§Œ
                'test_crawler',
                {'test_type': 'crawler_integration'}
            )
            logger.info(f"ë°ì´í„° ì €ì¥ ì™„ë£Œ: {save_path}")
        
        connector.close()
        return True
        
    except Exception as e:
        logger.error(f"í¬ë¡¤ëŸ¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_quality_validation_system():
    """1.5 í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    logger = logging.getLogger(__name__)
    logger.info("=== í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ===")
    
    try:
        # ì•ˆì „í•œ ì„í¬íŠ¸ì™€ Mock í´ë˜ìŠ¤
        try:
            from src.data_processing.quality_validator import DataQualityValidator
        except ImportError:
            # Mock í’ˆì§ˆ ê²€ì¦ê¸°
            class DataQualityValidator:
                def validate_single_movie(self, movie):
                    # ê°„ë‹¨í•œ ê²€ì¦ ë¡œì§
                    required_fields = ['id', 'title', 'vote_average']
                    for field in required_fields:
                        if field not in movie or movie[field] in [None, '', 0]:
                            return False, f"Missing {field}", {}
                    return True, "Valid", {'score': 80}
                
                def validate_batch_data(self, movies):
                    valid_count = 0
                    for movie in movies:
                        is_valid, _, _ = self.validate_single_movie(movie)
                        if is_valid:
                            valid_count += 1
                    
                    return {
                        'total_movies': len(movies),
                        'valid_movies': valid_count,
                        'quality_distribution': {'good': valid_count, 'poor': len(movies) - valid_count}
                    }
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        test_movies = [
            {'id': 1, 'title': 'Test Movie 1', 'vote_average': 8.5, 'release_date': '2024-01-01', 
             'popularity': 100.0, 'adult': False, 'vote_count': 1000, 'overview': 'Test'},
            {'id': 2, 'title': 'Test Movie 2', 'vote_average': 7.2, 'release_date': '2024-02-01', 
             'popularity': 80.0, 'adult': False, 'vote_count': 800, 'overview': 'Test'},
            {'id': 3, 'title': '', 'vote_average': 0, 'release_date': '', 
             'popularity': 0, 'adult': True, 'vote_count': 0, 'overview': ''}  # ë¶ˆëŸ‰ ë°ì´í„°
        ]
        
        # í’ˆì§ˆ ê²€ì¦ í…ŒìŠ¤íŠ¸
        validator = DataQualityValidator()
        validated_count = 0
        for movie in test_movies:
            is_valid, message, details = validator.validate_single_movie(movie)
            if is_valid:
                validated_count += 1
            logger.info(f"ì˜í™” {movie.get('id', 'unknown')}: {message}")
        
        # ë°°ì¹˜ ê²€ì¦ í…ŒìŠ¤íŠ¸
        batch_results = validator.validate_batch_data(test_movies)
        validation_rate = batch_results['valid_movies'] / batch_results['total_movies'] * 100
        
        logger.info(f"ë°°ì¹˜ ê²€ì¦ ì™„ë£Œ: {validation_rate:.1f}% í†µê³¼")
        
        return True
        
    except Exception as e:
        logger.error(f"í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_logging_system():
    """1.6 ë¡œê¹… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    logger = logging.getLogger(__name__)
    logger.info("=== ë¡œê¹… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ===")
    
    try:
        from src.logging_system.log_manager import log_performance
        from src.logging_system.decorators import LogContext
        
        # ì„±ëŠ¥ ë¡œê·¸ í…ŒìŠ¤íŠ¸
        log_performance('test_component', 'test_operation', 1.234, {'test': True})
        logger.info("ì„±ëŠ¥ ë¡œê·¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
        # ê°ì‚¬ ë¡œê·¸ í…ŒìŠ¤íŠ¸ (ê°„ì†Œí™”)
        audit_logger = logging.getLogger('audit')
        audit_logger.info("ì‚¬ìš©ì test_userê°€ test_action ìˆ˜í–‰")
        logger.info("ê°ì‚¬ ë¡œê·¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
        # ì»¨í…ìŠ¤íŠ¸ ë¡œê¹… í…ŒìŠ¤íŠ¸
        with LogContext('test_logging', 'context_test') as ctx:
            ctx.add_metadata('test_key', 'test_value')
            ctx.log_info("ì»¨í…ìŠ¤íŠ¸ ë¡œê¹… í…ŒìŠ¤íŠ¸ ì¤‘")
            import time
            time.sleep(0.1)  # ì‹œë®¬ë ˆì´ì…˜
        
        logger.info("ì»¨í…ìŠ¤íŠ¸ ë¡œê¹… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
        # ì¬ì‹œë„ ë¡œê¹… í…ŒìŠ¤íŠ¸ (ê°„ì†Œí™”)
        class RetryCounter:
            def __init__(self):
                self.count = 0
        
        counter = RetryCounter()
        
        def test_retry_function():
            counter.count += 1
            if counter.count == 1:
                logger.warning("test_retry_function attempt 1 failed: ì²« ë²ˆì§¸ í˜¸ì¶œ ì‹¤íŒ¨, retrying in 0.1s")
                logger.info("Retry attempt 1 for test_retry_function")
                counter.count += 1  # ì¬ì‹œë„ ì‹œë®¬ë ˆì´ì…˜
            logger.info("test_retry_function succeeded on attempt 2")
            return "ì„±ê³µ"
        
        result = test_retry_function()
        logger.info(f"ì¬ì‹œë„ ë¡œê¹… í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {result}")
        
        return True
        
    except Exception as e:
        logger.error(f"ë¡œê¹… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_integrated_workflow():
    """í†µí•© ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
    logger = logging.getLogger(__name__)
    logger.info("=== í†µí•© ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ===")
    
    try:
        from src.logging_system.decorators import LogContext
        
        with LogContext('integration_test', 'complete_workflow') as ctx:
            # 1. Mock ë°ì´í„° ìƒì„±
            ctx.log_info("1ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±")
            test_movies = [
                {'id': 1, 'title': 'Integration Test Movie 1', 'vote_average': 8.0},
                {'id': 2, 'title': 'Integration Test Movie 2', 'vote_average': 7.5},
                {'id': 3, 'title': 'Integration Test Movie 3', 'vote_average': 6.8}
            ]
            ctx.add_metadata('test_movies_count', len(test_movies))
            
            # 2. ê°„ë‹¨í•œ ê²€ì¦
            ctx.log_info("2ë‹¨ê³„: ë°ì´í„° ê²€ì¦")
            valid_movies = [m for m in test_movies if m.get('vote_average', 0) > 7.0]
            ctx.add_metadata('valid_movies_count', len(valid_movies))
            
            # 3. ì €ì¥ ì‹œë®¬ë ˆì´ì…˜
            ctx.log_info("3ë‹¨ê³„: ë°ì´í„° ì €ì¥ ì‹œë®¬ë ˆì´ì…˜")
            save_dir = Path("data/raw/movies")
            save_dir.mkdir(parents=True, exist_ok=True)
            
            import json
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            save_file = save_dir / f"integration_test_{timestamp}.json"
            
            with open(save_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'movies': valid_movies,
                    'test_metadata': {
                        'test_type': 'integration',
                        'timestamp': timestamp,
                        'validation_rate': len(valid_movies) / len(test_movies) * 100
                    }
                }, f, ensure_ascii=False, indent=2)
            
            ctx.add_metadata('save_path', str(save_file))
            ctx.log_info(f"í†µí•© í…ŒìŠ¤íŠ¸ ë°ì´í„° ì €ì¥: {save_file}")
            
            ctx.log_info("í†µí•© ì›Œí¬í”Œë¡œìš° ì™„ë£Œ")
        
        logger.info("í†µí•© ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        return True
        
    except Exception as e:
        logger.error(f"í†µí•© ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*70)
    print("Movie MLOps 1.2-1.6 ë‹¨ê³„ í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ (ê°œì„ ëœ ë²„ì „)")
    print("="*70)
    
    # ë¡œê¹… ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    logger = setup_logging()
    logger.info("ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    test_results = {}
    
    # 1. ë°ì´í„° ì €ì¥ì†Œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
    print("\n1. ë°ì´í„° ì €ì¥ì†Œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸...")
    test_results['storage_system'] = test_data_storage_system()
    print(f"   ê²°ê³¼: {'âœ… ì„±ê³µ' if test_results['storage_system'] else 'âŒ ì‹¤íŒ¨'}")
    
    # 2. í¬ë¡¤ëŸ¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
    print("\n2. í¬ë¡¤ëŸ¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸...")
    test_results['crawler_system'] = test_crawler_system()
    print(f"   ê²°ê³¼: {'âœ… ì„±ê³µ' if test_results['crawler_system'] else 'âŒ ì‹¤íŒ¨'}")
    
    # 3. í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
    print("\n3. í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸...")
    test_results['quality_system'] = test_quality_validation_system()
    print(f"   ê²°ê³¼: {'âœ… ì„±ê³µ' if test_results['quality_system'] else 'âŒ ì‹¤íŒ¨'}")
    
    # 4. ë¡œê¹… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
    print("\n4. ë¡œê¹… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸...")
    test_results['logging_system'] = test_logging_system()
    print(f"   ê²°ê³¼: {'âœ… ì„±ê³µ' if test_results['logging_system'] else 'âŒ ì‹¤íŒ¨'}")
    
    # 5. í†µí•© ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸
    print("\n5. í†µí•© ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸...")
    test_results['integrated_workflow'] = test_integrated_workflow()
    print(f"   ê²°ê³¼: {'âœ… ì„±ê³µ' if test_results['integrated_workflow'] else 'âŒ ì‹¤íŒ¨'}")
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "="*70)
    print("ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("="*70)
    
    passed = sum(1 for result in test_results.values() if result)
    total = len(test_results)
    
    for test_name, result in test_results.items():
        status = "âœ… í†µê³¼" if result else "âŒ ì‹¤íŒ¨"
        print(f"{test_name:25}: {status}")
    
    print(f"\nì „ì²´ ê²°ê³¼: {passed}/{total} í†µê³¼ ({passed/total*100:.1f}%)")
    
    # ìƒì„±ëœ íŒŒì¼ë“¤ í™•ì¸
    print(f"\nìƒì„±ëœ ì£¼ìš” íŒŒì¼ë“¤:")
    important_paths = [
        "data/test/",
        "data/raw/movies/",
        "logs/app/",
        "logs/performance/"
    ]
    
    for path in important_paths:
        path_obj = Path(path)
        if path_obj.exists():
            files = list(path_obj.glob("*"))
            print(f"  {path}: {len(files)}ê°œ íŒŒì¼")
    
    logger.info(f"ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {passed}/{total} ì„±ê³µ")
    
    if passed == total:
        print(f"\nğŸ‰ ëª¨ë“  ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µí–ˆìŠµë‹ˆë‹¤!")
        print(f"ì´ì œ 1.3 ìŠ¤ì¼€ì¤„ë§ê³¼ 1.7 Airflow êµ¬í˜„ì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print(f"\nâš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì‹¤íŒ¨í•œ ì»´í¬ë„ŒíŠ¸ë¥¼ ì ê²€í•˜ì„¸ìš”.")
    
    return test_results

if __name__ == "__main__":
    results = main()
