# 1.4 데이터 저장소 설정 - WSL Ubuntu 24.04 구현 가이드

## 📋 단계 개요

**목표**: 효율적이고 확장 가능한 데이터 저장 시스템 구축

**환경**: WSL Ubuntu 24.04 + Docker + Python 3.11

**핵심 가치**: 체계적인 데이터 관리를 통한 ML 파이프라인의 안정적 운영

---

## 🎯 1.4.1 파일 시스템 기반 저장 구조

### 목표
확장 가능하고 관리하기 쉬운 계층적 데이터 저장 구조 구축

### 완전한 디렉토리 구조 구현

```bash
# 완전한 데이터 저장소 구조 생성
docker exec mlops-dev bash -c "
mkdir -p data/raw/movies/{daily,weekly,monthly,genre,trending}
mkdir -p data/raw/metadata/{schemas,quality_reports,collection_stats}
mkdir -p data/processed/{features,aggregated,cleaned}
mkdir -p data/processed/features/{movie_features,user_features,interaction_features}
mkdir -p data/backup/{daily,weekly,monthly}
mkdir -p data/archive/{2024,2025}/{Q1,Q2,Q3,Q4}
mkdir -p data/test/{unit_test,integration_test,sample_data}
mkdir -p data/staging/{validation,preprocessing,quality_check}

echo '데이터 저장소 구조 생성 완료'
tree data/ -L 3
"
```

**최종 디렉토리 구조**:
```
data/
├── raw/                           # 원본 데이터
│   ├── movies/                    # 영화 데이터
│   │   ├── daily/                 # 일일 수집 데이터
│   │   ├── weekly/                # 주간 수집 데이터
│   │   ├── monthly/               # 월간 수집 데이터
│   │   ├── genre/                 # 장르별 데이터
│   │   └── trending/              # 트렌딩 데이터
│   └── metadata/                  # 메타데이터
│       ├── schemas/               # 데이터 스키마
│       ├── quality_reports/       # 품질 리포트
│       └── collection_stats/      # 수집 통계
├── processed/                     # 처리된 데이터
│   ├── features/                  # 피처 데이터
│   │   ├── movie_features/        # 영화 피처
│   │   ├── user_features/         # 사용자 피처
│   │   └── interaction_features/  # 상호작용 피처
│   ├── aggregated/                # 집계 데이터
│   └── cleaned/                   # 정제된 데이터
├── backup/                        # 백업 데이터
│   ├── daily/                     # 일일 백업
│   ├── weekly/                    # 주간 백업
│   └── monthly/                   # 월간 백업
├── archive/                       # 아카이브
│   ├── 2024/                      # 연도별
│   └── 2025/
├── test/                          # 테스트 데이터
│   ├── unit_test/                 # 단위 테스트용
│   ├── integration_test/          # 통합 테스트용
│   └── sample_data/               # 샘플 데이터
└── staging/                       # 스테이징 영역
    ├── validation/                # 검증 대기
    ├── preprocessing/             # 전처리 대기
    └── quality_check/             # 품질 검사 대기
```

### 파일 포맷 및 명명 규칙

#### **파일 명명 규칙**
```python
# data/naming_convention.py
from datetime import datetime

class DataFileNamingConvention:
    """데이터 파일 명명 규칙"""
    
    @staticmethod
    def daily_collection(date=None):
        """일일 수집 파일명"""
        if date is None:
            date = datetime.now()
        return f"daily_movies_{date.strftime('%Y%m%d')}.json"
    
    @staticmethod
    def weekly_collection(year, week):
        """주간 수집 파일명"""
        return f"weekly_movies_{year}_W{week:02d}.json"
    
    @staticmethod
    def genre_collection(genre_name, date=None):
        """장르별 수집 파일명"""
        if date is None:
            date = datetime.now()
        return f"genre_{genre_name}_{date.strftime('%Y%m%d')}.json"
    
    @staticmethod
    def quality_report(collection_type, date=None):
        """품질 리포트 파일명"""
        if date is None:
            date = datetime.now()
        return f"quality_report_{collection_type}_{date.strftime('%Y%m%d_%H%M%S')}.json"
    
    @staticmethod
    def backup_file(original_file, backup_type):
        """백업 파일명"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        return f"backup_{backup_type}_{timestamp}_{original_file}"
```

#### **파일 포맷 전략**
```python
# data/file_formats.py
import json
import pandas as pd
import gzip
from pathlib import Path

class DataFileManager:
    """데이터 파일 관리"""
    
    def save_json(self, data, filepath, compress=False):
        """JSON 형태 저장"""
        filepath = Path(filepath)
        
        if compress:
            with gzip.open(f"{filepath}.gz", 'wt', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2, default=str)
        else:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2, default=str)
    
    def save_csv(self, data, filepath, compress=False):
        """CSV 형태 저장"""
        df = pd.DataFrame(data)
        filepath = Path(filepath)
        
        if compress:
            df.to_csv(f"{filepath}.gz", index=False, compression='gzip')
        else:
            df.to_csv(filepath, index=False)
    
    def save_parquet(self, data, filepath):
        """Parquet 형태 저장 (대용량 데이터용)"""
        df = pd.DataFrame(data)
        df.to_parquet(filepath, index=False, compression='snappy')
    
    def load_data(self, filepath):
        """데이터 로드 (자동 포맷 감지)"""
        filepath = Path(filepath)
        
        if filepath.suffix == '.json':
            if filepath.name.endswith('.gz'):
                with gzip.open(filepath, 'rt', encoding='utf-8') as f:
                    return json.load(f)
            else:
                with open(filepath, 'r', encoding='utf-8') as f:
                    return json.load(f)
        
        elif filepath.suffix == '.csv':
            if filepath.name.endswith('.gz'):
                return pd.read_csv(filepath, compression='gzip')
            else:
                return pd.read_csv(filepath)
        
        elif filepath.suffix == '.parquet':
            return pd.read_parquet(filepath)
        
        else:
            raise ValueError(f"지원하지 않는 파일 형식: {filepath.suffix}")
    
    def get_file_info(self, filepath):
        """파일 정보 조회"""
        filepath = Path(filepath)
        if not filepath.exists():
            return None
        
        stat = filepath.stat()
        return {
            'size_bytes': stat.st_size,
            'size_mb': stat.st_size / 1024 / 1024,
            'created': datetime.fromtimestamp(stat.st_ctime),
            'modified': datetime.fromtimestamp(stat.st_mtime),
            'format': filepath.suffix
        }
```

---

## ✅ 완료 기준

### 1.4.1 기능적 완료 기준
- [x] 계층적 디렉토리 구조 구축 완료 ✅
- [x] 파일 명명 규칙 및 포맷 시스템 구현 ✅
- [ ] 데이터 버전 관리 시스템 구현
- [ ] 메타데이터 관리 시스템 구현
- [ ] 자동 백업 시스템 구현

### 1.4.2 기술적 완료 기준
- [ ] 백업 무결성 검증 시스템 작동
- [ ] 아카이브 자동화 로직 구현
- [ ] 데이터베이스 연동 (선택사항) 완료
- [ ] 압축 및 용량 최적화 구현
- [ ] 복원 및 롤백 기능 검증

### 1.4.3 운영적 완료 기준
- [ ] 일일/주간/월간 백업 자동 실행
- [ ] 저장 공간 모니터링 및 알림
- [ ] 데이터 무결성 정기 검사
- [ ] 아카이브 정책 자동 적용
- [ ] 재해 복구 절차 수립

---

## 🚀 다음 단계 준비

### 1.5 데이터 품질 검증으로 연계
- 저장된 데이터의 품질 검증 시스템 구축
- 이상 데이터 자동 감지 및 격리
- 품질 리포트 자동 생성

### 모니터링 시스템 통합
- 저장소 용량 및 성능 모니터링
- 백업/아카이브 상태 추적
- 데이터 접근 패턴 분석

**🎯 목표 달성**: 효율적이고 안정적인 엔터프라이즈급 데이터 저장소 시스템 구축 완료!

**다음 단계**: 1.5 데이터 품질 검증으로 저장된 데이터의 품질 보장 시스템 구현
