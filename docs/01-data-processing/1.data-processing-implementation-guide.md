---
title: "1단계: 데이터 처리 및 분석 도구들 - 상세 구현 가이드"
description: "MLOps 시스템의 기초가 되는 견고한 데이터 파이프라인 구축을 위한 완전한 구현 가이드"
stage: "01-data-processing"
phase: "implementation"
category: "mlops-pipeline"
difficulty: "intermediate"
estimated_time: "24-33 hours"
tags:
  - data-processing
  - tmdb-api
  - data-pipeline
  - quality-validation
  - automation
  - scheduling
  - logging
  - airflow
  - docker
  - wsl-ubuntu
authors:
  - mlops-team
last_updated: "2025-06-06"
version: "1.0"
status: "active"
prerequisites:
  - "WSL Ubuntu 24.04"
  - "Docker"
  - "Python 3.11"
  - "TMDB API Key"
outcomes:
  - "안정적인 TMDB 데이터 수집 시스템"
  - "자동화된 품질 검증 파이프라인"
  - "스케줄링 기반 데이터 갱신"
  - "종합적인 로깅 및 모니터링"
related_docs:
  - "implementation/1.1-data-source-connection-wsl-ubuntu-guide.md"
  - "implementation/1.2-data-crawler-development-guide.md"
  - "testing/2.comprehensive-testing-guide.md"
---

# 1단계: 데이터 처리 및 분석 도구들 - 상세 구현 가이드

## 📋 단계 개요

**목표**: 안정적인 데이터 파이프라인 구축하여 원시 데이터를 수집하고 처리

**핵심 가치**: 모든 ML 시스템의 기초가 되는 견고한 데이터 파이프라인 구축

---

## 🎯 1.1 데이터 소스 연결

### 목표
외부 API와 안정적인 연결을 통해 실시간 데이터 수집 체계 구축

### 상세 구현 사항

#### **1.1.1 TMDB API 연동 설정**
- **API 키 발급 및 관리**
  - TMDB 개발자 계정 생성
  - API 키 발급 (v3 API 사용)
  - 환경변수를 통한 안전한 키 관리
  - API 호출 제한 확인 (일일 요청 한도)

- **API 엔드포인트 분석**
  - `/movie/popular`: 인기 영화 목록
  - `/discover/movie`: 조건별 영화 검색
  - `/movie/{movie_id}`: 특정 영화 상세 정보
  - 페이지네이션 파라미터 이해

- **네트워크 설정**
  - 타임아웃 설정 (30초)
  - 재시도 로직 구현 (최대 3회)
  - User-Agent 헤더 설정
  - HTTPS 통신 보안 확인

#### **1.1.2 연결 안정성 확보**
- **에러 처리 전략**
  - HTTP 상태 코드별 대응
  - Rate Limiting 감지 및 대기
  - 네트워크 장애 시 재연결
  - 잘못된 응답 데이터 필터링

- **모니터링 체계**
  - API 응답 시간 측정
  - 성공/실패 비율 추적
  - 일일 API 사용량 모니터링
  - 장애 발생 시 알림 체계

---

## 🎯 1.2 데이터 크롤러 개발

### 목표
TMDBCrawler 클래스를 기반으로 확장 가능한 데이터 수집 시스템 구축

### 상세 구현 사항

#### **1.2.1 TMDBCrawler 클래스 설계**
- **기본 구조**
  ```python
  class TMDBCrawler:
      def __init__(self, region="KR", language="ko-KR"):
          # 설정 초기화
      
      def get_popular_movies(self, page):
          # 단일 페이지 수집
      
      def get_bulk_popular_movies(self, start_page, end_page):
          # 다중 페이지 수집
      
      def validate_movie_data(self, movie):
          # 데이터 유효성 검증
  ```

- **핵심 기능**
  - 페이지별 데이터 수집
  - 요청 간격 제어 (API 제한 준수)
  - 데이터 검증 및 정제
  - 중복 데이터 제거

#### **1.2.2 데이터 수집 전략**
- **수집 범위 설정**
  - 인기 영화: 1-100 페이지 (2000개 영화)
  - 장르별 영화: 주요 장르 5개
  - 최신 영화: 최근 3개월
  - 평점 높은 영화: 평점 7.0 이상

- **수집 스케줄링**
  - 일일 수집: 신규 인기 영화
  - 주간 수집: 장르별 업데이트
  - 월간 수집: 전체 데이터 갱신
  - 실시간 수집: 트렌딩 영화

#### **1.2.3 데이터 품질 관리**
- **필수 필드 검증**
  - movie_id: 고유 식별자
  - title: 영화 제목
  - release_date: 출시일
  - vote_average: 평점
  - popularity: 인기도

- **데이터 정제 규칙**
  - 성인 영화 필터링
  - 개봉 전 영화 제외
  - 평점 0인 영화 제외
  - 중복 제목 처리

---

## 🎯 1.3 데이터 수집 스케줄링

### 목표
자동화된 데이터 수집으로 지속적인 데이터 업데이트 보장

### 상세 구현 사항

#### **1.3.1 Cron 기반 스케줄링**
- **스케줄 설정**
  ```bash
  # 매일 새벽 2시 인기 영화 수집
  0 2 * * * /path/to/collect_popular_movies.py
  
  # 매주 일요일 새벽 3시 전체 데이터 갱신
  0 3 * * 0 /path/to/full_data_refresh.py
  
  # 매시간 트렌딩 데이터 수집
  0 * * * * /path/to/collect_trending.py
  ```

- **스케줄 모니터링**
  - 작업 실행 로그 기록
  - 실패 시 재시도 로직
  - 작업 완료 알림
  - 실행 시간 추적

#### **1.3.2 Python 기반 스케줄러**
- **schedule 라이브러리 활용**
  ```python
  import schedule
  import time
  
  def job():
      crawler = TMDBCrawler()
      crawler.collect_daily_data()
  
  schedule.every().day.at("02:00").do(job)
  schedule.every().hour.do(collect_trending)
  ```

- **백그라운드 실행**
  - 데몬 프로세스로 실행
  - 시스템 재시작 시 자동 실행
  - 프로세스 상태 모니터링
  - 메모리 사용량 관리

---

## 🎯 1.4 데이터 저장소 설정

### 목표
효율적이고 확장 가능한 데이터 저장 시스템 구축

### 상세 구현 사항

#### **1.4.1 파일 시스템 기반 저장**
- **디렉토리 구조**
  ```
  data/
  ├── raw/                    # 원본 데이터
  │   ├── movies/
  │   │   ├── 2025-06-03/    # 날짜별 폴더
  │   │   └── latest/        # 최신 데이터 링크
  │   └── trending/
  ├── processed/              # 처리된 데이터
  └── archive/               # 아카이브
  ```

- **파일 포맷 선택**
  - JSON: 원본 API 응답 저장
  - CSV: 정제된 테이블 데이터
  - Parquet: 대용량 데이터 압축 저장
  - 백업: 일일 tar.gz 압축

#### **1.4.2 데이터베이스 연동 (선택사항)**
- **SQLite 로컬 DB**
  - 개발/테스트 환경용
  - 단일 파일 관리
  - 트랜잭션 지원
  - 간단한 쿼리 인터페이스

- **PostgreSQL 연동**
  - 프로덕션 환경용
  - 동시성 지원
  - 복잡한 쿼리 최적화
  - 백업/복구 체계

#### **1.4.3 데이터 버전 관리**
- **파일 버전 관리**
  - 타임스탬프 기반 파일명
  - 증분 업데이트 추적
  - 변경 사항 로그
  - 롤백 기능

- **메타데이터 관리**
  - 수집 시간 기록
  - 데이터 소스 정보
  - 품질 지표 저장
  - 스키마 변경 추적

---

## 🎯 1.5 데이터 품질 검증

### 목표
수집된 데이터의 품질을 보장하고 이상 데이터를 조기에 감지

### 상세 구현 사항

#### **1.5.1 실시간 품질 검증**
- **필드별 검증 규칙**
  - 필수 필드 존재 확인
  - 데이터 타입 검증
  - 값 범위 확인 (평점: 0-10)
  - 형식 검증 (날짜, URL 등)

- **통계적 이상 탐지**
  - 평점 분포 모니터링
  - 인기도 지수 변화 추적
  - 수집량 급변 감지
  - 중복 데이터 비율 확인

#### **1.5.2 배치 품질 분석**
- **일간 품질 리포트**
  - 수집 완료율
  - 필드별 누락율
  - 데이터 품질 점수
  - 이상 패턴 탐지

- **품질 개선 액션**
  - 자동 데이터 정제
  - 이상 데이터 격리
  - 재수집 트리거
  - 품질 임계값 알림

---

## 🎯 1.6 로깅 시스템 구축

### 목표
체계적인 로그 관리로 시스템 운영 가시성 확보

### 상세 구현 사항

#### **1.6.1 로그 레벨 설계**
- **로그 분류**
  - DEBUG: 상세 디버깅 정보
  - INFO: 일반 운영 정보
  - WARNING: 주의 필요 상황
  - ERROR: 오류 발생
  - CRITICAL: 심각한 시스템 오류

- **로그 내용**
  - API 호출 기록
  - 데이터 수집 통계
  - 오류 상세 정보
  - 성능 지표

#### **1.6.2 로그 저장 및 관리**
- **로그 파일 구조**
  ```
  logs/
  ├── app/
  │   ├── crawler-2025-06-03.log
  │   └── data-quality-2025-06-03.log
  ├── error/
  │   └── error-2025-06-03.log
  └── archive/
      └── 2025-05/
  ```

- **로그 순환 관리**
  - 일별 로그 파일 분리
  - 자동 압축 및 아카이브
  - 보존 기간 설정 (30일)
  - 디스크 용량 모니터링

#### **1.6.3 로그 분석 도구**
- **기본 분석**
  - grep/awk 활용 로그 검색
  - 오류 패턴 분석
  - 성능 트렌드 추적
  - 알림 조건 설정

- **시각화 (선택사항)**
  - 로그 대시보드 구축
  - 실시간 로그 모니터링
  - 이상 패턴 시각화
  - 성능 지표 차트

---

## 🎯 1.7 Apache Airflow 기초 설정 (선택사항)

### 목표
워크플로우 오케스트레이션을 위한 Airflow 기초 환경 구축

### 상세 구현 사항

#### **1.7.1 Airflow 설치 및 설정**
- **설치 과정**
  - Python 가상환경 설정
  - Apache Airflow 설치
  - 데이터베이스 초기화
  - 기본 설정 파일 구성

- **기본 구성**
  - airflow.cfg 설정
  - 데이터베이스 연결
  - 웹서버 포트 설정
  - 스케줄러 설정

#### **1.7.2 간단한 DAG 작성**
- **데이터 수집 DAG**
  ```python
  from airflow import DAG
  from airflow.operators.python import PythonOperator
  
  def collect_tmdb_data():
      # TMDB 데이터 수집 로직
      pass
  
  dag = DAG(
      'tmdb_data_collection',
      schedule_interval='@daily',
      start_date=datetime(2025, 6, 1)
  )
  
  collect_task = PythonOperator(
      task_id='collect_tmdb_data',
      python_callable=collect_tmdb_data,
      dag=dag
  )
  ```

- **DAG 구성 요소**
  - Task 정의
  - 의존성 설정
  - 스케줄 설정
  - 실패 처리

---

## ✅ 완료 기준

### 1.7.1 기능적 완료 기준
- [ ] TMDB API에서 안정적으로 데이터 수집 (성공률 95% 이상)
- [ ] 일일 최소 1000개 영화 데이터 수집
- [ ] 데이터 품질 검증 통과율 90% 이상
- [ ] 수집 프로세스 자동화 완료
- [ ] 로그 시스템으로 모든 작업 추적 가능

### 1.7.2 기술적 완료 기준
- [ ] TMDBCrawler 클래스 구현 완료
- [ ] 에러 처리 및 재시도 로직 작동
- [ ] 데이터 저장소 구조화 완료
- [ ] 스케줄링 시스템 안정 운영
- [ ] 품질 검증 자동화 구현

### 1.7.3 운영적 완료 기준
- [ ] 시스템 24시간 무인 운영 가능
- [ ] 장애 발생 시 자동 복구
- [ ] 데이터 백업 및 복구 체계 구축
- [ ] 성능 모니터링 대시보드 구축
- [ ] 팀원 대상 운영 매뉴얼 작성

---

## 🚀 다음 단계 준비

### 2단계 연계 작업
- 수집된 원본 데이터를 2단계 피처 엔지니어링 입력으로 활용
- 데이터 품질 지표를 피처 스토어 메타데이터로 전달
- 로깅 시스템을 전체 MLOps 파이프라인으로 확장

### 개선 계획
- 실시간 스트리밍 데이터 수집 고려
- 다중 데이터 소스 통합 계획
- 분산 처리 시스템 도입 검토
- 클라우드 네이티브 아키텍처 전환

이 1단계를 완료하면 견고한 데이터 기반 위에서 MLOps 시스템을 구축할 수 있는 기초가 마련됩니다.
