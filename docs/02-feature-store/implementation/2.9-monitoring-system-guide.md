---
title: "2.9 모니터링 시스템 구축 가이드"
description: "Prometheus, Grafana, ELK Stack을 활용한 종합 모니터링 시스템"
author: "MLOps Team"
created: "2025-06-06"
updated: "2025-06-06"
version: "1.0"
stage: "2.9"
category: "Monitoring & Observability"
tags: ["Prometheus", "Grafana", "ELK", "모니터링", "알림", "대시보드"]
prerequisites: ["2.8 파이프라인 자동화 완료", "Docker", "시계열 데이터베이스"]
difficulty: "advanced"
estimated_time: "10-12시간"
improvement_type: "monitoring"
priority: "high"
---

# 2.9 모니터링 시스템 구축 가이드

## 📋 개요

**목표**: 현재 구현된 MLOps 시스템에 종합적인 모니터링 및 관측성 시스템 추가

**개선 배경**: 프로덕션 환경에서 안정적 운영을 위한 실시간 모니터링 필요

---

## 🔧 ELK Stack 로그 관리

### Logstash 파이프라인 설정

```ruby
# config/logstash/pipeline/mlops-logs.conf
input {
  beats {
    port => 5044
  }
  
  file {
    path => "/var/log/mlops/*.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
  }
}

filter {
  # TMDB 크롤러 로그 파싱
  if [source] =~ "tmdb_crawler" {
    grok {
      match => { 
        "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{DATA:logger} - %{GREEDYDATA:message}"
      }
    }
    
    # 수집 통계 추출
    if [message] =~ "수집 완료" {
      grok {
        match => {
          "message" => "수집 완료: %{NUMBER:movies_collected:int}개"
        }
      }
    }
  }
  
  # 피처 엔지니어링 로그 파싱
  if [source] =~ "tmdb_processor" {
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} - %{DATA:feature_type} 피처 %{NUMBER:feature_count:int}개 생성"
      }
    }
  }
  
  # API 서버 로그 파싱
  if [source] =~ "api_server" {
    grok {
      match => {
        "message" => "%{IPORHOST:clientip} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] \"%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}\" %{NUMBER:response:int} (?:-|%{NUMBER:bytes:int}) %{QS:referrer} %{QS:agent} %{NUMBER:request_time:float}"
      }
    }
  }
  
  # 에러 로그 특별 처리
  if [level] == "ERROR" {
    mutate {
      add_tag => ["error"]
      add_field => { "alert_required" => "true" }
    }
  }
  
  # 성능 지표 추출
  if [message] =~ "duration" {
    grok {
      match => {
        "message" => "duration: %{NUMBER:duration:float}ms"
      }
    }
  }
  
  # 날짜 파싱
  date {
    match => [ "timestamp", "ISO8601" ]
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "mlops-logs-%{+YYYY.MM.dd}"
  }
  
  # 에러 로그는 별도 인덱스로
  if "error" in [tags] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "mlops-errors-%{+YYYY.MM.dd}"
    }
  }
  
  # 실시간 알림을 위한 출력
  if [alert_required] == "true" {
    http {
      url => "http://alertmanager:9093/api/v1/alerts"
      http_method => "post"
      content_type => "application/json"
      format => "json"
    }
  }
  
  stdout { codec => rubydebug }
}
```

### Elasticsearch 인덱스 템플릿

```json
{
  "index_patterns": ["mlops-logs-*"],
  "template": {
    "settings": {
      "number_of_shards": 1,
      "number_of_replicas": 1,
      "index.refresh_interval": "5s"
    },
    "mappings": {
      "properties": {
        "@timestamp": { "type": "date" },
        "level": { "type": "keyword" },
        "logger": { "type": "keyword" },
        "message": { "type": "text" },
        "source": { "type": "keyword" },
        "movies_collected": { "type": "integer" },
        "feature_count": { "type": "integer" },
        "feature_type": { "type": "keyword" },
        "duration": { "type": "float" },
        "response": { "type": "integer" },
        "request_time": { "type": "float" },
        "clientip": { "type": "ip" },
        "verb": { "type": "keyword" },
        "request": { "type": "text" }
      }
    }
  }
}
```

### Kibana 대시보드

```json
{
  "version": "8.0.0",
  "objects": [
    {
      "id": "mlops-overview-dashboard",
      "type": "dashboard",
      "attributes": {
        "title": "MLOps 로그 분석 대시보드",
        "panelsJSON": "[{\"version\":\"8.0.0\",\"gridData\":{\"x\":0,\"y\":0,\"w\":24,\"h\":15},\"panelIndex\":\"1\",\"embeddableConfig\":{},\"panelRefName\":\"panel_1\"}]",
        "optionsJSON": "{\"useMargins\":true,\"syncColors\":false,\"hidePanelTitles\":false}",
        "timeRestore": false,
        "kibanaSavedObjectMeta": {
          "searchSourceJSON": "{\"query\":{\"query\":\"\",\"language\":\"kuery\"},\"filter\":[]}"
        }
      }
    },
    {
      "id": "error-analysis-visualization",
      "type": "visualization",
      "attributes": {
        "title": "에러 분석",
        "visState": "{\"title\":\"에러 분석\",\"type\":\"histogram\",\"params\":{\"grid\":{\"categoryLines\":false,\"style\":{\"color\":\"#eee\"}},\"categoryAxes\":[{\"id\":\"CategoryAxis-1\",\"type\":\"category\",\"position\":\"bottom\",\"show\":true,\"style\":{},\"scale\":{\"type\":\"linear\"},\"labels\":{\"show\":true,\"truncate\":100},\"title\":{}}],\"valueAxes\":[{\"id\":\"ValueAxis-1\",\"name\":\"LeftAxis-1\",\"type\":\"value\",\"position\":\"left\",\"show\":true,\"style\":{},\"scale\":{\"type\":\"linear\",\"mode\":\"normal\"},\"labels\":{\"show\":true,\"rotate\":0,\"filter\":false,\"truncate\":100},\"title\":{\"text\":\"Count\"}}],\"seriesParams\":[{\"show\":\"true\",\"type\":\"histogram\",\"mode\":\"stacked\",\"data\":{\"label\":\"Count\",\"id\":\"1\"},\"valueAxis\":\"ValueAxis-1\",\"drawLinesBetweenPoints\":true,\"showCircles\":true}],\"addTooltip\":true,\"addLegend\":true,\"legendPosition\":\"right\",\"times\":[],\"addTimeMarker\":false},\"aggs\":[{\"id\":\"1\",\"enabled\":true,\"type\":\"count\",\"schema\":\"metric\",\"params\":{}},{\"id\":\"2\",\"enabled\":true,\"type\":\"terms\",\"schema\":\"segment\",\"params\":{\"field\":\"level\",\"size\":5,\"order\":\"desc\",\"orderBy\":\"1\"}}]}",
        "uiStateJSON": "{}",
        "kibanaSavedObjectMeta": {
          "searchSourceJSON": "{\"index\":\"mlops-logs-*\",\"query\":{\"match\":{\"level\":\"ERROR\"}},\"filter\":[]}"
        }
      }
    }
  ]
}
```

---

## 🔧 지능형 알림 시스템

### Alertmanager 설정

```yaml
# config/alertmanager/alertmanager.yml
global:
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'mlops-alerts@company.com'
  smtp_auth_username: 'mlops-alerts@company.com'
  smtp_auth_password: 'your_password'

# 알림 라우팅 규칙
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'default'
  routes:
  - match:
      severity: critical
    receiver: 'critical-alerts'
  - match:
      severity: warning
    receiver: 'warning-alerts'
  - match:
      team: data-science
    receiver: 'data-team'

# 알림 수신자 설정
receivers:
- name: 'default'
  email_configs:
  - to: 'mlops-team@company.com'
    subject: 'MLOps Alert: {{ .GroupLabels.alertname }}'
    body: |
      {{ range .Alerts }}
      Alert: {{ .Annotations.summary }}
      Description: {{ .Annotations.description }}
      Time: {{ .StartsAt }}
      {{ end }}

- name: 'critical-alerts'
  email_configs:
  - to: 'oncall@company.com'
    subject: '🚨 CRITICAL: {{ .GroupLabels.alertname }}'
  slack_configs:
  - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
    channel: '#mlops-alerts'
    title: '🚨 Critical MLOps Alert'
    text: |
      {{ range .Alerts }}
      *Alert:* {{ .Annotations.summary }}
      *Description:* {{ .Annotations.description }}
      *Severity:* {{ .Labels.severity }}
      {{ end }}

- name: 'data-team'
  slack_configs:
  - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
    channel: '#data-science'
    title: 'Data Pipeline Alert'

# 알림 억제 규칙
inhibit_rules:
- source_match:
    severity: 'critical'
  target_match:
    severity: 'warning'
  equal: ['alertname', 'cluster', 'service']
```

### Prometheus 알림 규칙

```yaml
# config/prometheus/alert_rules.yml
groups:
- name: mlops.rules
  rules:
  
  # 데이터 파이프라인 알림
  - alert: DataCollectionFailure
    expr: increase(tmdb_collection_errors_total[1h]) > 3
    for: 5m
    labels:
      severity: critical
      team: data-science
    annotations:
      summary: "TMDB 데이터 수집 실패 증가"
      description: "지난 1시간 동안 {{ $value }}회 데이터 수집 실패"
      
  - alert: FeatureGenerationSlow
    expr: histogram_quantile(0.95, rate(feature_generation_duration_seconds_bucket[5m])) > 300
    for: 10m
    labels:
      severity: warning
      team: data-science
    annotations:
      summary: "피처 생성 성능 저하"
      description: "피처 생성 시간이 5분을 초과함"
      
  - alert: DataQualityDegraded
    expr: data_completeness_ratio < 0.9
    for: 5m
    labels:
      severity: warning
      team: data-science
    annotations:
      summary: "데이터 품질 저하"
      description: "데이터 완전성이 {{ $value }}로 저하됨"
      
  # 모델 성능 알림
  - alert: ModelAccuracyDrop
    expr: model_accuracy_score < 0.75
    for: 15m
    labels:
      severity: critical
      team: ml-engineering
    annotations:
      summary: "모델 정확도 급락"
      description: "{{ $labels.model_name }} 모델 정확도가 {{ $value }}로 하락"
      
  - alert: ModelDriftDetected
    expr: feature_drift_score > 0.1
    for: 5m
    labels:
      severity: warning
      team: ml-engineering
    annotations:
      summary: "피처 드리프트 감지"
      description: "{{ $labels.feature_name }}에서 드리프트 감지 (점수: {{ $value }})"
      
  # 시스템 성능 알림
  - alert: HighAPILatency
    expr: histogram_quantile(0.95, rate(api_request_duration_seconds_bucket[5m])) > 1
    for: 5m
    labels:
      severity: warning
      team: platform
    annotations:
      summary: "API 응답 지연"
      description: "{{ $labels.endpoint }} API 응답시간이 1초 초과"
      
  - alert: LowCacheHitRate
    expr: feature_store_cache_hits_total / (feature_store_cache_hits_total + feature_store_cache_misses_total) < 0.7
    for: 10m
    labels:
      severity: warning
      team: platform
    annotations:
      summary: "캐시 적중률 저하"
      description: "피처 스토어 캐시 적중률이 {{ $value }}로 저하"
      
  - alert: HighMemoryUsage
    expr: memory_usage_bytes / (1024*1024*1024) > 8
    for: 5m
    labels:
      severity: critical
      team: platform
    annotations:
      summary: "메모리 사용량 임계치 초과"
      description: "{{ $labels.service_name }} 메모리 사용량: {{ $value }}GB"
      
  # 비즈니스 메트릭 알림
  - alert: RecommendationPerformanceDrop
    expr: recommendation_click_rate < 0.05
    for: 30m
    labels:
      severity: warning
      team: product
    annotations:
      summary: "추천 성과 저하"
      description: "추천 클릭률이 {{ $value }}로 하락"
      
  - alert: ConversionRateDrop
    expr: conversion_rate < 0.02
    for: 1h
    labels:
      severity: warning
      team: product
    annotations:
      summary: "전환율 저하"
      description: "{{ $labels.channel }} 채널 전환율이 {{ $value }}로 하락"
```

---

## 🔧 자동화된 이상 탐지

### 기계학습 기반 이상 탐지

```python
# src/monitoring/anomaly_detector.py
import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from typing import Dict, List, Any, Tuple
import logging
from datetime import datetime, timedelta

class MLOpsAnomalyDetector:
    """MLOps 메트릭 이상 탐지 시스템"""
    
    def __init__(self, metrics_collector: MLOpsMetricsCollector):
        self.metrics_collector = metrics_collector
        self.logger = logging.getLogger("AnomalyDetector")
        
        # 이상 탐지 모델들
        self.models = {
            'feature_metrics': IsolationForest(contamination=0.1, random_state=42),
            'model_metrics': IsolationForest(contamination=0.05, random_state=42),
            'system_metrics': IsolationForest(contamination=0.1, random_state=42)
        }
        
        self.scalers = {
            'feature_metrics': StandardScaler(),
            'model_metrics': StandardScaler(),
            'system_metrics': StandardScaler()
        }
        
        # 학습 데이터 저장
        self.training_data = {}
        self.is_trained = False
    
    def collect_training_data(self, days: int = 7) -> None:
        """이상 탐지 모델 훈련을 위한 데이터 수집"""
        self.logger.info(f"이상 탐지 훈련 데이터 수집 시작: {days}일")
        
        # Prometheus에서 메트릭 데이터 수집
        from prometheus_api_client import PrometheusConnect
        
        prom = PrometheusConnect(url="http://prometheus:9090", disable_ssl=True)
        end_time = datetime.now()
        start_time = end_time - timedelta(days=days)
        
        # 피처 메트릭 수집
        feature_queries = [
            'feature_generation_duration_seconds',
            'feature_drift_score',
            'feature_staleness_hours'
        ]
        
        feature_data = []
        for query in feature_queries:
            data = prom.get_metric_range_data(
                metric_name=query,
                start_time=start_time,
                end_time=end_time,
                step='5m'
            )
            if data:
                values = [float(point[1]) for point in data[0]['values']]
                feature_data.extend(values)
        
        if feature_data:
            self.training_data['feature_metrics'] = np.array(feature_data).reshape(-1, 1)
        
        # 모델 메트릭 수집
        model_queries = [
            'model_prediction_latency_seconds',
            'model_accuracy_score',
            'prediction_confidence_score'
        ]
        
        model_data = []
        for query in model_queries:
            data = prom.get_metric_range_data(
                metric_name=query,
                start_time=start_time,
                end_time=end_time,
                step='5m'
            )
            if data:
                values = [float(point[1]) for point in data[0]['values']]
                model_data.extend(values)
        
        if model_data:
            self.training_data['model_metrics'] = np.array(model_data).reshape(-1, 1)
        
        # 시스템 메트릭 수집
        system_queries = [
            'cpu_usage_percent',
            'memory_usage_bytes',
            'api_request_duration_seconds'
        ]
        
        system_data = []
        for query in system_queries:
            data = prom.get_metric_range_data(
                metric_name=query,
                start_time=start_time,
                end_time=end_time,
                step='5m'
            )
            if data:
                values = [float(point[1]) for point in data[0]['values']]
                system_data.extend(values)
        
        if system_data:
            self.training_data['system_metrics'] = np.array(system_data).reshape(-1, 1)
        
        self.logger.info(f"훈련 데이터 수집 완료: {len(self.training_data)} 카테고리")
    
    def train_models(self) -> None:
        """이상 탐지 모델 훈련"""
        if not self.training_data:
            raise ValueError("훈련 데이터가 없습니다. collect_training_data()를 먼저 실행하세요.")
        
        self.logger.info("이상 탐지 모델 훈련 시작")
        
        for category, data in self.training_data.items():
            if len(data) < 100:  # 최소 데이터 요구사항
                self.logger.warning(f"{category}: 훈련 데이터 부족 ({len(data)}개)")
                continue
            
            # 데이터 정규화
            scaled_data = self.scalers[category].fit_transform(data)
            
            # 모델 훈련
            self.models[category].fit(scaled_data)
            
            self.logger.info(f"{category} 이상 탐지 모델 훈련 완료")
        
        self.is_trained = True
    
    def detect_anomalies(self, current_metrics: Dict[str, float]) -> Dict[str, Any]:
        """실시간 이상 탐지"""
        if not self.is_trained:
            raise ValueError("모델이 훈련되지 않았습니다. train_models()를 먼저 실행하세요.")
        
        anomalies = {}
        
        for category, metrics in current_metrics.items():
            if category not in self.models:
                continue
            
            # 메트릭 배열로 변환
            metric_array = np.array(list(metrics.values())).reshape(1, -1)
            
            # 정규화
            scaled_metrics = self.scalers[category].transform(metric_array)
            
            # 이상 탐지
            anomaly_score = self.models[category].decision_function(scaled_metrics)[0]
            is_anomaly = self.models[category].predict(scaled_metrics)[0] == -1
            
            anomalies[category] = {
                'is_anomaly': is_anomaly,
                'anomaly_score': anomaly_score,
                'metrics': metrics,
                'timestamp': datetime.now().isoformat()
            }
            
            if is_anomaly:
                self.logger.warning(f"이상 탐지: {category} (점수: {anomaly_score:.3f})")
                self._send_anomaly_alert(category, anomalies[category])
        
        return anomalies
    
    def _send_anomaly_alert(self, category: str, anomaly_info: Dict[str, Any]):
        """이상 탐지 알림 전송"""
        message = f"""
        🚨 MLOps 이상 탐지!
        
        카테고리: {category}
        이상 점수: {anomaly_info['anomaly_score']:.3f}
        감지 시간: {anomaly_info['timestamp']}
        
        관련 메트릭:
        {anomaly_info['metrics']}
        
        즉시 확인이 필요합니다.
        """
        
        # Slack으로 알림 전송 (실제 환경에서 구현)
        self.logger.critical(message)


class HealthChecker:
    """시스템 건강 상태 체크"""
    
    def __init__(self):
        self.logger = logging.getLogger("HealthChecker")
        
        # 건강 상태 기준
        self.health_thresholds = {
            'api_response_time': 1.0,      # 1초 이하
            'cache_hit_rate': 0.8,         # 80% 이상
            'data_freshness_hours': 2,     # 2시간 이내
            'error_rate': 0.01,            # 1% 이하
            'memory_usage_gb': 8,          # 8GB 이하
            'cpu_usage_percent': 80        # 80% 이하
        }
    
    def check_system_health(self) -> Dict[str, Any]:
        """시스템 전체 건강 상태 체크"""
        
        health_status = {
            'timestamp': datetime.now().isoformat(),
            'overall_health': 'healthy',
            'components': {},
            'issues': []
        }
        
        # 각 컴포넌트 건강 상태 체크
        components = [
            ('api_server', self._check_api_health),
            ('feature_store', self._check_feature_store_health),
            ('data_pipeline', self._check_data_pipeline_health),
            ('model_serving', self._check_model_serving_health)
        ]
        
        unhealthy_count = 0
        
        for component_name, check_func in components:
            try:
                component_health = check_func()
                health_status['components'][component_name] = component_health
                
                if component_health['status'] != 'healthy':
                    unhealthy_count += 1
                    health_status['issues'].extend(component_health.get('issues', []))
                    
            except Exception as e:
                health_status['components'][component_name] = {
                    'status': 'error',
                    'error': str(e)
                }
                unhealthy_count += 1
        
        # 전체 건강 상태 결정
        if unhealthy_count == 0:
            health_status['overall_health'] = 'healthy'
        elif unhealthy_count <= 2:
            health_status['overall_health'] = 'degraded'
        else:
            health_status['overall_health'] = 'unhealthy'
        
        # 심각한 상태일 때 알림
        if health_status['overall_health'] == 'unhealthy':
            self._send_health_alert(health_status)
        
        return health_status
    
    def _check_api_health(self) -> Dict[str, Any]:
        """API 서버 건강 상태 체크"""
        import requests
        
        try:
            # API 응답 시간 체크
            start_time = time.time()
            response = requests.get('http://localhost:8001/health', timeout=5)
            response_time = time.time() - start_time
            
            status = 'healthy'
            issues = []
            
            if response.status_code != 200:
                status = 'unhealthy'
                issues.append(f"API 상태 코드: {response.status_code}")
            
            if response_time > self.health_thresholds['api_response_time']:
                status = 'degraded'
                issues.append(f"응답 시간 초과: {response_time:.2f}초")
            
            return {
                'status': status,
                'response_time': response_time,
                'status_code': response.status_code,
                'issues': issues
            }
            
        except Exception as e:
            return {
                'status': 'unhealthy',
                'error': str(e),
                'issues': ['API 서버 연결 실패']
            }
    
    def _check_feature_store_health(self) -> Dict[str, Any]:
        """피처 스토어 건강 상태 체크"""
        from src.features.store.feature_store import SimpleFeatureStore
        
        try:
            store = SimpleFeatureStore()
            stats = store.get_store_stats()
            
            status = 'healthy'
            issues = []
            
            # 캐시 적중률 체크
            cache_stats = stats.get('cache_stats', {})
            hit_rate = cache_stats.get('hit_ratio', 0)
            
            if hit_rate < self.health_thresholds['cache_hit_rate']:
                status = 'degraded'
                issues.append(f"캐시 적중률 저하: {hit_rate:.2f}")
            
            return {
                'status': status,
                'cache_hit_rate': hit_rate,
                'total_features': stats.get('total_features', 0),
                'issues': issues
            }
            
        except Exception as e:
            return {
                'status': 'unhealthy',
                'error': str(e),
                'issues': ['피처 스토어 연결 실패']
            }
    
    def _send_health_alert(self, health_status: Dict[str, Any]):
        """건강 상태 알림 전송"""
        message = f"""
        🚨 시스템 건강 상태 경고!
        
        전체 상태: {health_status['overall_health']}
        감지 시간: {health_status['timestamp']}
        
        문제점:
        {chr(10).join(health_status['issues'])}
        
        즉시 점검이 필요합니다.
        """
        
        self.logger.critical(message)
```

---

## 🔧 Docker Compose 통합 설정

### 모니터링 스택 배포

```yaml
# docker-compose.monitoring.yml
version: '3.8'

services:
  # Prometheus
  prometheus:
    image: prom/prometheus:v2.40.0
    container_name: mlops-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus:/etc/prometheus
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
    networks:
      - mlops-network

  # Grafana
  grafana:
    image: grafana/grafana:9.3.0
    container_name: mlops-grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
      - GF_INSTALL_PLUGINS=grafana-piechart-panel
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./config/grafana/datasources:/etc/grafana/provisioning/datasources
    networks:
      - mlops-network
    depends_on:
      - prometheus

  # Elasticsearch
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.5.0
    container_name: mlops-elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    networks:
      - mlops-network

  # Logstash
  logstash:
    image: docker.elastic.co/logstash/logstash:8.5.0
    container_name: mlops-logstash
    ports:
      - "5044:5044"
      - "9600:9600"
    volumes:
      - ./config/logstash/pipeline:/usr/share/logstash/pipeline
      - ./config/logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml
      - /var/log/mlops:/var/log/mlops
    networks:
      - mlops-network
    depends_on:
      - elasticsearch

  # Kibana
  kibana:
    image: docker.elastic.co/kibana/kibana:8.5.0
    container_name: mlops-kibana
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    networks:
      - mlops-network
    depends_on:
      - elasticsearch

  # Alertmanager
  alertmanager:
    image: prom/alertmanager:v0.25.0
    container_name: mlops-alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./config/alertmanager:/etc/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=http://localhost:9093'
    networks:
      - mlops-network

  # Node Exporter (시스템 메트릭)
  node-exporter:
    image: prom/node-exporter:v1.5.0
    container_name: mlops-node-exporter
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    networks:
      - mlops-network

networks:
  mlops-network:
    driver: bridge

volumes:
  prometheus_data:
  grafana_data:
  elasticsearch_data:
```

---

## ✅ 완료 기준

### 기능적 완료 기준
- [ ] Prometheus 메트릭 수집 정상 동작 확인
- [ ] Grafana 대시보드 구축 및 시각화 완료
- [ ] ELK Stack 로그 수집 및 분석 구현
- [ ] 지능형 알림 시스템 동작 확인
- [ ] 이상 탐지 시스템 구현 및 테스트

### 기술적 완료 기준
- [ ] 메트릭 수집 간격 5분 이하
- [ ] 알림 전송 시간 1분 이내
- [ ] 대시보드 응답 시간 3초 이하
- [ ] 로그 검색 성능 최적화
- [ ] 이상 탐지 정확도 90% 이상

### 운영 완료 기준
- [ ] 24/7 모니터링 시스템 운영
- [ ] 장애 예측 및 사전 대응 체계 구축
- [ ] 모니터링 대시보드 교육 완료
- [ ] 운영 가이드 및 문서 작성
- [ ] 백업 및 복구 절차 검증

---

## 🚀 다음 단계

완료 후 [2.10 MLflow 통합 강화](./2.10-mlflow-integration-enhancement.md)로 진행하여 실험 추적과 모델 관리를 고도화합니다.

---

## 📚 참고 자료

- [Prometheus Monitoring Guide](https://prometheus.io/docs/introduction/overview/)
- [Grafana Dashboard Best Practices](https://grafana.com/docs/)
- [ELK Stack Configuration](https://www.elastic.co/guide/)
- [MLOps Monitoring Patterns](https://ml-ops.org/content/monitoring-ml-systems)
