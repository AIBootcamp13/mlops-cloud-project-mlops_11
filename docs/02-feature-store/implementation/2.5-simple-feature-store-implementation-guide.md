---
title: "2.5 간단한 피처 스토어 구현 가이드"
description: "로컬 파일 시스템 기반의 경량 피처 스토어 구축"
author: "MLOps Team"
created: "2025-06-05"
updated: "2025-06-05"
version: "1.0"
stage: "2.5"
category: "Feature Store"
tags: ["피처스토어", "파일시스템", "API인터페이스", "캐싱", "성능최적화"]
prerequisites: ["2.4 메타데이터 관리 완료", "Parquet", "pandas", "API 설계"]
difficulty: "intermediate"
estimated_time: "6-8시간"
---

# 2.5 간단한 피처 스토어 구현 가이드

## 📋 개요

**목표**: 로컬 파일 시스템 기반의 경량 피처 스토어 구축

**핵심 가치**: 복잡한 인프라 없이도 효율적인 피처 관리와 서빙 시스템 제공

---

## 🎯 학습 목표

### 이론적 목표
- 피처 스토어 아키텍처 패턴 이해
- 온라인/오프라인 서빙의 차이점 학습
- 캐싱 전략과 성능 최적화 방법 습득

### 실무적 목표
- 파일 기반 피처 스토어 설계 및 구현
- RESTful API 인터페이스 개발
- 캐싱 시스템 및 성능 최적화 구현

---

## 🔧 2.5.1 파일 기반 스토리지 설계

### 디렉토리 구조

```
feature_store/
├── features/                           # 피처 데이터 저장소
│   ├── content/                        # 컨텐츠 관련 피처
│   │   ├── movie_genres.parquet        # 영화 장르 피처
│   │   ├── movie_ratings.parquet       # 영화 평점 피처
│   │   └── movie_popularity.parquet    # 영화 인기도 피처
│   ├── user/                           # 사용자 관련 피처
│   │   ├── user_preferences.parquet    # 사용자 선호도 피처
│   │   └── user_demographics.parquet   # 사용자 인구통계 피처
│   └── interaction/                    # 상호작용 피처
│       └── user_movie_interactions.parquet
├── metadata/                           # 메타데이터 저장소
├── registry/                           # 피처 레지스트리
├── cache/                              # 캐시 저장소
└── config/                             # 설정 파일
```

### 최적화된 스토리지 구현

```python
import pandas as pd
import pyarrow.parquet as pq
from pathlib import Path
from typing import Dict, List, Any, Optional

class OptimizedStorage:
    """최적화된 파일 저장 관리자"""
    
    def __init__(self, base_path: str):
        self.base_path = Path(base_path)
        self.compression_config = {
            'engine': 'pyarrow',
            'compression': 'snappy',
            'index': False
        }
        
    def save_feature_data(self, data: pd.DataFrame, 
                         feature_group: str, 
                         feature_name: str,
                         partition_cols: Optional[List[str]] = None) -> str:
        """피처 데이터 저장 (Parquet 최적화)"""
        
        # 저장 경로 생성
        storage_path = self.base_path / "features" / feature_group
        storage_path.mkdir(parents=True, exist_ok=True)
        
        file_path = storage_path / f"{feature_name}.parquet"
        
        # 데이터 타입 최적화
        optimized_data = self._optimize_dtypes(data)
        
        # 데이터 저장
        optimized_data.to_parquet(file_path, **self.compression_config)
        
        return str(file_path)
    
    def load_feature_data(self, feature_group: str, 
                         feature_name: str,
                         columns: Optional[List[str]] = None,
                         filters: Optional[List] = None) -> pd.DataFrame:
        """피처 데이터 로드 (선택적 로딩)"""
        
        file_path = self.base_path / "features" / feature_group / f"{feature_name}.parquet"
        
        try:
            return pd.read_parquet(
                file_path,
                columns=columns,
                filters=filters,
                engine='pyarrow'
            )
        except FileNotFoundError:
            print(f"피처 데이터를 찾을 수 없음: {feature_name}")
            return pd.DataFrame()
    
    def _optimize_dtypes(self, df: pd.DataFrame) -> pd.DataFrame:
        """데이터 타입 최적화"""
        
        optimized_df = df.copy()
        
        for col in optimized_df.columns:
            col_data = optimized_df[col]
            
            # 수치형 데이터 최적화
            if pd.api.types.is_integer_dtype(col_data):
                col_min = col_data.min()
                col_max = col_data.max()
                
                if col_min >= 0:  # 음수가 없는 경우
                    if col_max < 255:
                        optimized_df[col] = col_data.astype('uint8')
                    elif col_max < 65535:
                        optimized_df[col] = col_data.astype('uint16')
                else:  # 음수가 있는 경우
                    if col_min > -128 and col_max < 127:
                        optimized_df[col] = col_data.astype('int8')
                    elif col_min > -32768 and col_max < 32767:
                        optimized_df[col] = col_data.astype('int16')
            
            elif pd.api.types.is_float_dtype(col_data):
                # 부동소수점 최적화
                if col_data.between(-3.4e38, 3.4e38).all():
                    optimized_df[col] = col_data.astype('float32')
            
            elif pd.api.types.is_object_dtype(col_data):
                # 문자열 최적화
                unique_count = col_data.nunique()
                total_count = len(col_data)
                
                # 카테고리형으로 변환할지 결정
                if unique_count / total_count < 0.5:
                    optimized_df[col] = col_data.astype('category')
        
        return optimized_df
    
    def get_storage_info(self, feature_group: str, 
                        feature_name: str) -> Dict[str, Any]:
        """스토리지 정보 조회"""
        
        file_path = self.base_path / "features" / feature_group / f"{feature_name}.parquet"
        
        info = {
            'feature_group': feature_group,
            'feature_name': feature_name,
            'exists': file_path.exists(),
            'file_size_mb': 0,
            'row_count': 0,
            'column_count': 0
        }
        
        if file_path.exists():
            info['file_size_mb'] = file_path.stat().st_size / 1024 / 1024
            
            try:
                parquet_file = pq.ParquetFile(file_path)
                info['row_count'] = parquet_file.metadata.num_rows
                info['column_count'] = len(parquet_file.schema)
            except Exception as e:
                info['error'] = str(e)
        
        return info
```

---

## 🔧 2.5.2 API 인터페이스 설계

### FastAPI 기반 RESTful API

```python
from fastapi import FastAPI, HTTPException, Query
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import uvicorn
from datetime import datetime

class FeatureRequest(BaseModel):
    """피처 요청 모델"""
    feature_names: List[str]
    entity_ids: Optional[List[str]] = None
    timestamp: Optional[str] = None

class SimpleFeatureStoreAPI:
    """간단한 피처 스토어 API"""
    
    def __init__(self, storage: OptimizedStorage):
        self.storage = storage
        self.app = FastAPI(title="Simple Feature Store API", version="1.0.0")
        self._setup_routes()
    
    def _setup_routes(self):
        """API 라우트 설정"""
        
        @self.app.get("/health")
        async def health_check():
            """헬스 체크"""
            return {"status": "healthy", "timestamp": datetime.now().isoformat()}
        
        @self.app.get("/features/{feature_group}/{feature_name}")
        async def get_feature_data(
            feature_group: str,
            feature_name: str,
            columns: Optional[str] = Query(None, description="Comma-separated column names"),
            limit: Optional[int] = Query(None, description="Limit number of rows"),
            offset: Optional[int] = Query(0, description="Offset for pagination")
        ):
            """피처 데이터 조회"""
            
            try:
                # 컬럼 파싱
                selected_columns = None
                if columns:
                    selected_columns = [col.strip() for col in columns.split(',')]
                
                # 데이터 로드
                data = self.storage.load_feature_data(
                    feature_group=feature_group,
                    feature_name=feature_name,
                    columns=selected_columns
                )
                
                if data.empty:
                    raise HTTPException(status_code=404, detail="Feature data not found")
                
                # 페이지네이션 적용
                if limit:
                    data = data.iloc[offset:offset+limit]
                
                # JSON 변환
                result = {
                    "feature_group": feature_group,
                    "feature_name": feature_name,
                    "data": data.to_dict('records'),
                    "metadata": {
                        "total_rows": len(data),
                        "columns": list(data.columns),
                        "dtypes": {col: str(dtype) for col, dtype in data.dtypes.items()}
                    }
                }
                
                return result
                
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.post("/features/{feature_group}/{feature_name}")
        async def save_feature_data(
            feature_group: str,
            feature_name: str,
            data: Dict[str, Any]
        ):
            """피처 데이터 저장"""
            
            try:
                # DataFrame 변환
                df = pd.DataFrame(data['records'])
                
                # 데이터 저장
                file_path = self.storage.save_feature_data(
                    data=df,
                    feature_group=feature_group,
                    feature_name=feature_name
                )
                
                return {
                    "message": "Feature data saved successfully",
                    "file_path": file_path,
                    "rows_saved": len(df)
                }
                
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/features/{feature_group}/{feature_name}/info")
        async def get_feature_info(feature_group: str, feature_name: str):
            """피처 정보 조회"""
            
            try:
                storage_info = self.storage.get_storage_info(feature_group, feature_name)
                return {"storage_info": storage_info}
                
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.post("/features/batch")
        async def get_batch_features(request: FeatureRequest):
            """배치 피처 조회"""
            
            try:
                result = {}
                
                for feature_name in request.feature_names:
                    # 피처 그룹 추정
                    feature_group = self._infer_feature_group(feature_name)
                    
                    data = self.storage.load_feature_data(
                        feature_group=feature_group,
                        feature_name=feature_name
                    )
                    
                    # 엔티티 ID 필터링
                    if request.entity_ids and 'entity_id' in data.columns:
                        data = data[data['entity_id'].isin(request.entity_ids)]
                    
                    result[feature_name] = data.to_dict('records')
                
                return {
                    "features": result,
                    "metadata": {
                        "request_timestamp": request.timestamp or datetime.now().isoformat(),
                        "feature_count": len(request.feature_names)
                    },
                    "timestamp": datetime.now().isoformat()
                }
                
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
    
    def _infer_feature_group(self, feature_name: str) -> str:
        """피처 이름에서 그룹 추정"""
        
        if 'user' in feature_name.lower():
            return 'user'
        elif 'movie' in feature_name.lower() or 'content' in feature_name.lower():
            return 'content'
        elif 'interaction' in feature_name.lower():
            return 'interaction'
        else:
            return 'misc'
    
    def run(self, host: str = "0.0.0.0", port: int = 8001):
        """API 서버 실행"""
        uvicorn.run(self.app, host=host, port=port)
```

### 클라이언트 SDK

```python
import requests
from typing import List, Dict, Any, Optional

class FeatureStoreClient:
    """피처 스토어 클라이언트 SDK"""
    
    def __init__(self, base_url: str = "http://localhost:8001"):
        self.base_url = base_url.rstrip('/')
        self.session = requests.Session()
    
    def get_feature(self, feature_group: str, feature_name: str,
                   columns: Optional[List[str]] = None,
                   limit: Optional[int] = None,
                   offset: int = 0) -> Dict[str, Any]:
        """단일 피처 데이터 조회"""
        
        url = f"{self.base_url}/features/{feature_group}/{feature_name}"
        
        params = {'offset': offset}
        if columns:
            params['columns'] = ','.join(columns)
        if limit:
            params['limit'] = limit
        
        response = self.session.get(url, params=params)
        response.raise_for_status()
        
        return response.json()
    
    def save_feature(self, feature_group: str, feature_name: str,
                    data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """피처 데이터 저장"""
        
        url = f"{self.base_url}/features/{feature_group}/{feature_name}"
        
        payload = {'records': data}
        
        response = self.session.post(url, json=payload)
        response.raise_for_status()
        
        return response.json()
    
    def get_batch_features(self, feature_names: List[str],
                          entity_ids: Optional[List[str]] = None) -> Dict[str, Any]:
        """배치 피처 조회"""
        
        url = f"{self.base_url}/features/batch"
        
        payload = {
            'feature_names': feature_names,
            'entity_ids': entity_ids
        }
        
        response = self.session.post(url, json=payload)
        response.raise_for_status()
        
        return response.json()
    
    def get_feature_info(self, feature_group: str, feature_name: str) -> Dict[str, Any]:
        """피처 정보 조회"""
        
        url = f"{self.base_url}/features/{feature_group}/{feature_name}/info"
        
        response = self.session.get(url)
        response.raise_for_status()
        
        return response.json()
```

---

## 🔧 2.5.3 캐싱 전략

### 메모리 캐시 구현

```python
import time
from typing import Any, Optional
from collections import OrderedDict

class LRUCache:
    """LRU (Least Recently Used) 캐시"""
    
    def __init__(self, max_size: int = 1000, ttl_seconds: int = 3600):
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
        self.cache = OrderedDict()
        self.timestamps = {}
    
    def get(self, key: str) -> Optional[Any]:
        """캐시에서 값 조회"""
        
        if key not in self.cache:
            return None
        
        # TTL 확인
        if time.time() - self.timestamps[key] > self.ttl_seconds:
            self.delete(key)
            return None
        
        # LRU 업데이트 (가장 최근 사용으로 이동)
        self.cache.move_to_end(key)
        return self.cache[key]
    
    def set(self, key: str, value: Any):
        """캐시에 값 저장"""
        
        if key in self.cache:
            # 기존 값 업데이트
            self.cache.move_to_end(key)
        else:
            # 새 값 추가
            if len(self.cache) >= self.max_size:
                # 가장 오래된 항목 제거
                oldest_key = next(iter(self.cache))
                self.delete(oldest_key)
        
        self.cache[key] = value
        self.timestamps[key] = time.time()
    
    def delete(self, key: str):
        """캐시에서 값 제거"""
        
        if key in self.cache:
            del self.cache[key]
            del self.timestamps[key]
    
    def clear(self):
        """캐시 초기화"""
        self.cache.clear()
        self.timestamps.clear()
    
    def get_stats(self) -> Dict[str, Any]:
        """캐시 통계 정보"""
        
        current_time = time.time()
        expired_count = sum(
            1 for timestamp in self.timestamps.values()
            if current_time - timestamp > self.ttl_seconds
        )
        
        return {
            'total_items': len(self.cache),
            'max_size': self.max_size,
            'expired_items': expired_count,
            'cache_usage_ratio': len(self.cache) / self.max_size,
            'average_age_seconds': (
                sum(current_time - ts for ts in self.timestamps.values()) / len(self.timestamps)
                if self.timestamps else 0
            )
        }

class CachedFeatureStore:
    """캐시 기능이 있는 피처 스토어"""
    
    def __init__(self, storage: OptimizedStorage, 
                 cache_size: int = 1000, cache_ttl: int = 3600):
        self.storage = storage
        self.cache = LRUCache(max_size=cache_size, ttl_seconds=cache_ttl)
        self.cache_hits = 0
        self.cache_misses = 0
    
    def get_feature_data(self, feature_group: str, feature_name: str,
                        columns: Optional[List[str]] = None) -> pd.DataFrame:
        """캐시를 활용한 피처 데이터 조회"""
        
        # 캐시 키 생성
        cache_key = self._generate_cache_key(feature_group, feature_name, columns)
        
        # 캐시에서 조회
        cached_data = self.cache.get(cache_key)
        
        if cached_data is not None:
            self.cache_hits += 1
            return cached_data
        
        # 캐시 미스 - 스토리지에서 로드
        self.cache_misses += 1
        data = self.storage.load_feature_data(
            feature_group=feature_group,
            feature_name=feature_name,
            columns=columns
        )
        
        # 캐시에 저장 (데이터가 있는 경우만)
        if not data.empty:
            self.cache.set(cache_key, data)
        
        return data
    
    def invalidate_cache(self, feature_group: str, feature_name: str):
        """특정 피처의 캐시 무효화"""
        
        # 해당 피처와 관련된 모든 캐시 키 찾기
        keys_to_remove = []
        for key in self.cache.cache.keys():
            if f"{feature_group}:{feature_name}" in key:
                keys_to_remove.append(key)
        
        # 캐시에서 제거
        for key in keys_to_remove:
            self.cache.delete(key)
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """캐시 통계 조회"""
        
        total_requests = self.cache_hits + self.cache_misses
        hit_rate = self.cache_hits / total_requests if total_requests > 0 else 0
        
        cache_stats = self.cache.get_stats()
        cache_stats.update({
            'cache_hits': self.cache_hits,
            'cache_misses': self.cache_misses,
            'hit_rate': hit_rate,
            'total_requests': total_requests
        })
        
        return cache_stats
    
    def _generate_cache_key(self, feature_group: str, feature_name: str,
                           columns: Optional[List[str]]) -> str:
        """캐시 키 생성"""
        
        key_parts = [feature_group, feature_name]
        
        if columns:
            key_parts.append(','.join(sorted(columns)))
        
        return ':'.join(key_parts)
```

### 디스크 캐시 구현

```python
import pickle
import hashlib
from pathlib import Path

class DiskCache:
    """디스크 기반 캐시"""
    
    def __init__(self, cache_dir: str = "cache/disk", max_size_mb: int = 1000):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.max_size_mb = max_size_mb
    
    def get(self, key: str) -> Optional[Any]:
        """캐시에서 값 조회"""
        
        cache_file = self._get_cache_file(key)
        
        if not cache_file.exists():
            return None
        
        try:
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        except Exception:
            # 손상된 캐시 파일 제거
            cache_file.unlink(missing_ok=True)
            return None
    
    def set(self, key: str, value: Any):
        """캐시에 값 저장"""
        
        cache_file = self._get_cache_file(key)
        
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(value, f)
            
            # 캐시 크기 확인 및 정리
            self._cleanup_if_needed()
            
        except Exception as e:
            print(f"디스크 캐시 저장 실패: {e}")
    
    def delete(self, key: str):
        """캐시에서 값 제거"""
        
        cache_file = self._get_cache_file(key)
        cache_file.unlink(missing_ok=True)
    
    def clear(self):
        """캐시 초기화"""
        
        for cache_file in self.cache_dir.glob("*.cache"):
            cache_file.unlink()
    
    def get_cache_size_mb(self) -> float:
        """캐시 크기 조회 (MB)"""
        
        total_size = sum(
            f.stat().st_size for f in self.cache_dir.glob("*.cache")
        )
        return total_size / 1024 / 1024
    
    def _get_cache_file(self, key: str) -> Path:
        """캐시 파일 경로 생성"""
        
        # 키를 해시하여 파일명 생성
        key_hash = hashlib.md5(key.encode()).hexdigest()
        return self.cache_dir / f"{key_hash}.cache"
    
    def _cleanup_if_needed(self):
        """필요시 캐시 정리"""
        
        current_size = self.get_cache_size_mb()
        
        if current_size > self.max_size_mb:
            # 오래된 파일부터 제거
            cache_files = list(self.cache_dir.glob("*.cache"))
            cache_files.sort(key=lambda f: f.stat().st_mtime)
            
            target_size = self.max_size_mb * 0.8  # 80%까지 정리
            
            for cache_file in cache_files:
                cache_file.unlink()
                current_size = self.get_cache_size_mb()
                
                if current_size <= target_size:
                    break
```

---

## 🔧 2.5.4 성능 최적화

### 통합 피처 스토어 구현

```python
class SimpleFeatureStore:
    """통합 피처 스토어"""
    
    def __init__(self, base_path: str, 
                 memory_cache_size: int = 1000,
                 disk_cache_size_mb: int = 1000):
        # 핵심 컴포넌트 초기화
        self.storage = OptimizedStorage(base_path)
        self.cached_store = CachedFeatureStore(
            self.storage, 
            cache_size=memory_cache_size
        )
        self.disk_cache = DiskCache(
            cache_dir=f"{base_path}/cache/disk",
            max_size_mb=disk_cache_size_mb
        )
        
        # API 서버
        self.api = SimpleFeatureStoreAPI(self.storage)
        
        # 성능 메트릭
        self.performance_metrics = {
            'total_requests': 0,
            'avg_response_time': 0,
            'error_count': 0
        }
    
    def get_features(self, feature_names: List[str],
                    entity_ids: Optional[List[str]] = None,
                    use_cache: bool = True) -> Dict[str, pd.DataFrame]:
        """다중 피처 조회 (최적화된 버전)"""
        
        start_time = time.time()
        results = {}
        
        try:
            for feature_name in feature_names:
                feature_group = self._infer_feature_group(feature_name)
                
                if use_cache:
                    # 캐시 우선 조회
                    data = self.cached_store.get_feature_data(
                        feature_group, feature_name
                    )
                else:
                    # 직접 스토리지 조회
                    data = self.storage.load_feature_data(
                        feature_group, feature_name
                    )
                
                # 엔티티 필터링
                if entity_ids and 'entity_id' in data.columns:
                    data = data[data['entity_id'].isin(entity_ids)]
                
                results[feature_name] = data
            
            # 성능 메트릭 업데이트
            response_time = time.time() - start_time
            self._update_metrics(response_time, success=True)
            
        except Exception as e:
            self._update_metrics(time.time() - start_time, success=False)
            raise
        
        return results
    
    def save_features(self, feature_group: str, 
                     features_data: Dict[str, pd.DataFrame]) -> Dict[str, str]:
        """다중 피처 저장"""
        
        results = {}
        
        for feature_name, data in features_data.items():
            try:
                # 스토리지에 저장
                file_path = self.storage.save_feature_data(
                    data, feature_group, feature_name
                )
                
                # 캐시 무효화
                self.cached_store.invalidate_cache(feature_group, feature_name)
                
                results[feature_name] = file_path
                
            except Exception as e:
                results[feature_name] = f"Error: {e}"
        
        return results
    
    def get_performance_report(self) -> Dict[str, Any]:
        """성능 리포트 생성"""
        
        cache_stats = self.cached_store.get_cache_stats()
        
        return {
            'performance_metrics': self.performance_metrics,
            'memory_cache_stats': cache_stats,
            'disk_cache_size_mb': self.disk_cache.get_cache_size_mb(),
            'storage_info': self._get_storage_summary()
        }
    
    def start_api_server(self, host: str = "0.0.0.0", port: int = 8001):
        """API 서버 시작"""
        self.api.run(host=host, port=port)
    
    def _infer_feature_group(self, feature_name: str) -> str:
        """피처 그룹 추정"""
        
        if 'user' in feature_name.lower():
            return 'user'
        elif 'movie' in feature_name.lower() or 'content' in feature_name.lower():
            return 'content'
        elif 'interaction' in feature_name.lower():
            return 'interaction'
        else:
            return 'misc'
    
    def _update_metrics(self, response_time: float, success: bool):
        """성능 메트릭 업데이트"""
        
        self.performance_metrics['total_requests'] += 1
        
        if success:
            # 이동 평균으로 응답 시간 업데이트
            total_requests = self.performance_metrics['total_requests']
            current_avg = self.performance_metrics['avg_response_time']
            new_avg = (current_avg * (total_requests - 1) + response_time) / total_requests
            self.performance_metrics['avg_response_time'] = new_avg
        else:
            self.performance_metrics['error_count'] += 1
    
    def _get_storage_summary(self) -> Dict[str, Any]:
        """스토리지 요약 정보"""
        
        features_path = self.storage.base_path / "features"
        
        summary = {
            'total_feature_groups': 0,
            'total_features': 0,
            'total_size_mb': 0
        }
        
        if features_path.exists():
            feature_groups = [d for d in features_path.iterdir() if d.is_dir()]
            summary['total_feature_groups'] = len(feature_groups)
            
            for group_dir in feature_groups:
                feature_files = list(group_dir.glob("*.parquet"))
                summary['total_features'] += len(feature_files)
                
                for file_path in feature_files:
                    summary['total_size_mb'] += file_path.stat().st_size / 1024 / 1024
        
        return summary
```

---

## ✅ 완료 기준

### 기능적 완료 기준
- [ ] 파일 기반 스토리지 시스템 구현 완료
- [ ] RESTful API 인터페이스 구현
- [ ] 클라이언트 SDK 개발
- [ ] 메모리 및 디스크 캐싱 시스템 구현
- [ ] 배치 피처 조회 기능 구현

### 기술적 완료 기준
- [ ] Parquet 파일 최적화 (압축률 50% 이상)
- [ ] API 응답 시간 100ms 이하 (캐시 적중 시)
- [ ] 캐시 적중률 80% 이상
- [ ] 동시 요청 처리 능력 (100 RPS 이상)
- [ ] 스토리지 공간 효율성 검증

### 품질 완료 기준
- [ ] API 문서화 (OpenAPI/Swagger) 완료
- [ ] 단위 테스트 및 통합 테스트 작성
- [ ] 성능 벤치마크 및 부하 테스트
- [ ] 에러 핸들링 및 복구 메커니즘 구현
- [ ] 사용자 가이드 및 예제 코드 작성

---

## 🚀 다음 단계

완료 후 [2.6 피처 버전 관리](./2.6-feature-version-control-guide.md)로 진행하여 피처의 진화와 호환성을 체계적으로 관리합니다.

---

## 📚 참고 자료

- [Apache Parquet Documentation](https://parquet.apache.org/docs/)
- [FastAPI User Guide](https://fastapi.tiangolo.com/)
- [Caching Strategies](https://aws.amazon.com/caching/)
- [Feature Store Design Patterns](https://www.featurestore.org/what-is-a-feature-store)
