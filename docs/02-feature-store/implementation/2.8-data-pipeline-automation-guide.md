---
title: "2.8 데이터 파이프라인 자동화 가이드"
description: "Apache Airflow를 활용한 TMDB 데이터 파이프라인 완전 자동화"
author: "MLOps Team"
created: "2025-06-06"
updated: "2025-06-06"
version: "1.0"
stage: "2.8"
category: "Pipeline Automation"
tags: ["Airflow", "자동화", "스케줄링", "데이터파이프라인", "모니터링"]
prerequisites: ["2.7 Feast 통합 완료", "Apache Airflow", "TMDB API"]
difficulty: "advanced"
estimated_time: "8-10시간"
improvement_type: "automation"
priority: "high"
---

# 2.8 데이터 파이프라인 자동화 가이드

## 📋 개요

**목표**: 현재 구현된 TMDB 크롤러와 피처 엔지니어링을 Apache Airflow로 완전 자동화

**개선 배경**: 수동 실행에서 벗어나 안정적이고 확장 가능한 자동화 시스템 구축

---

## 🎯 현재 상태 분석

### 구현된 컴포넌트
```python
# 현재 src/data_processing/tmdb_crawler.py에 구현됨
class TMDBCrawler:
    - get_popular_movies_bulk()      # ✅ 구현됨
    - get_movies_by_genre()          # ✅ 구현됨  
    - get_trending_movies()          # ✅ 구현됨
    - collect_comprehensive_dataset() # ✅ 구현됨
    
# 현재 src/features/engineering/tmdb_processor.py에 구현됨
class AdvancedTMDBPreProcessor:
    - extract_all_features()         # ✅ 구현됨
    - extract_temporal_features()    # ✅ 구현됨
    - extract_statistical_features() # ✅ 구현됨
```

### 부족한 부분
- ❌ 자동 스케줄링 없음
- ❌ 에러 복구 메커니즘 부족
- ❌ 데이터 품질 체크 자동화 없음
- ❌ 파이프라인 상태 모니터링 없음

---

## 🔧 Airflow DAG 구현

### 메인 데이터 수집 DAG

```python
# airflow/dags/tmdb_data_collection_dag.py
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.sensors.filesystem import FileSensor
from airflow.utils.dates import days_ago
import sys
from pathlib import Path

# 프로젝트 경로 추가
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.data_processing.tmdb_crawler import TMDBCrawler
from src.features.engineering.tmdb_processor import AdvancedTMDBPreProcessor
from src.features.store.feature_store import SimpleFeatureStore

# DAG 기본 설정
default_args = {
    'owner': 'mlops-team',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'max_active_runs': 1,
}

dag = DAG(
    'tmdb_data_collection_pipeline',
    default_args=default_args,
    description='TMDB 데이터 수집 및 피처 생성 파이프라인',
    schedule_interval='0 2 * * *',  # 매일 오전 2시
    catchup=False,
    tags=['tmdb', 'data-collection', 'features'],
)

def collect_tmdb_data(**context):
    """TMDB 데이터 수집"""
    crawler = TMDBCrawler()
    
    try:
        # 종합 데이터셋 수집
        results = crawler.collect_comprehensive_dataset()
        
        # 수집 통계 로깅
        stats = crawler.get_collection_stats()
        context['task_instance'].xcom_push(
            key='collection_stats', 
            value=stats
        )
        
        return results['collection_summary']
        
    except Exception as e:
        context['task_instance'].log.error(f"데이터 수집 실패: {e}")
        raise
    finally:
        crawler.close()

def validate_collected_data(**context):
    """수집된 데이터 품질 검증"""
    from src.data_processing.quality_validator import QualityValidator
    
    validator = QualityValidator()
    
    # 최신 수집 파일 검증
    latest_files = validator.get_latest_collection_files()
    validation_results = {}
    
    for file_path in latest_files:
        result = validator.validate_movie_data_file(file_path)
        validation_results[file_path] = result
        
        # 품질 기준 미달 시 실패
        if result['quality_score'] < 0.8:
            raise ValueError(f"데이터 품질 기준 미달: {file_path}")
    
    return validation_results

def process_features(**context):
    """피처 엔지니어링 실행"""
    from src.data_processing.quality_validator import QualityValidator
    
    # 검증된 데이터 로드
    validator = QualityValidator()
    movies_data = validator.load_validated_movies()
    
    # 피처 프로세서 초기화
    processor = AdvancedTMDBPreProcessor(movies_data)
    
    # 모든 피처 생성
    features = processor.extract_all_features()
    
    # 피처 검증
    validation_results = processor.validate_features(features)
    
    # 검증 실패 시 에러
    for category, results in validation_results.items():
        if not all(results.values()):
            raise ValueError(f"피처 검증 실패: {category}")
    
    context['task_instance'].xcom_push(
        key='feature_stats',
        value=validation_results
    )
    
    return features

def save_to_feature_store(**context):
    """피처 스토어에 저장"""
    # 이전 태스크에서 피처 데이터 받기
    features = context['task_instance'].xcom_pull(
        task_ids='process_features'
    )
    
    # 피처 스토어 초기화
    store = SimpleFeatureStore()
    
    # 피처별로 저장
    saved_paths = {}
    
    for category, df in features.items():
        if category != 'metadata':
            paths = store.save_features(
                feature_group='processed_movies',
                features_data={category: df}
            )
            saved_paths.update(paths)
    
    return saved_paths

def generate_quality_report(**context):
    """데이터 품질 리포트 생성"""
    from src.data_processing.quality_reporter import QualityReporter
    
    reporter = QualityReporter()
    
    # 수집 통계 가져오기
    collection_stats = context['task_instance'].xcom_pull(
        task_ids='collect_data',
        key='collection_stats'
    )
    
    # 피처 통계 가져오기
    feature_stats = context['task_instance'].xcom_pull(
        task_ids='process_features',
        key='feature_stats'
    )
    
    # 리포트 생성
    report = reporter.generate_daily_report(
        collection_stats=collection_stats,
        feature_stats=feature_stats,
        date=context['ds']
    )
    
    # 리포트 저장
    report_path = reporter.save_report(report, context['ds'])
    
    return report_path

# 태스크 정의
collect_data = PythonOperator(
    task_id='collect_data',
    python_callable=collect_tmdb_data,
    dag=dag,
)

validate_data = PythonOperator(
    task_id='validate_data',
    python_callable=validate_collected_data,
    dag=dag,
)

process_features = PythonOperator(
    task_id='process_features',
    python_callable=process_features,
    dag=dag,
)

save_features = PythonOperator(
    task_id='save_features',
    python_callable=save_to_feature_store,
    dag=dag,
)

quality_report = PythonOperator(
    task_id='quality_report',
    python_callable=generate_quality_report,
    dag=dag,
)

# 데이터 백업
backup_data = BashOperator(
    task_id='backup_data',
    bash_command='python scripts/backup_automation.py --type daily',
    dag=dag,
)

# 의존성 설정
collect_data >> validate_data >> process_features >> save_features >> quality_report >> backup_data
```

### 주간 모델 재훈련 DAG

```python
# airflow/dags/weekly_model_retrain_dag.py
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator

default_args = {
    'owner': 'mlops-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 6, 1),
    'email_on_failure': True,
    'retries': 2,
    'retry_delay': timedelta(minutes=10),
}

dag = DAG(
    'weekly_model_retrain_pipeline',
    default_args=default_args,
    description='주간 모델 재훈련 파이프라인',
    schedule_interval='0 3 * * 0',  # 매주 일요일 오전 3시
    catchup=False,
    tags=['model', 'training', 'weekly'],
)

def prepare_training_data(**context):
    """훈련 데이터 준비"""
    from src.features.store.feature_store import SimpleFeatureStore
    
    store = SimpleFeatureStore()
    
    # 최근 일주일 피처 데이터 조회
    end_date = context['ds']
    start_date = (datetime.strptime(end_date, '%Y-%m-%d') - timedelta(days=7)).strftime('%Y-%m-%d')
    
    features = store.get_features_by_date_range(
        feature_group='processed_movies',
        start_date=start_date,
        end_date=end_date
    )
    
    return len(features)

def train_recommendation_model(**context):
    """추천 모델 훈련"""
    # 기존 my-mlops의 MoviePredictor 활용
    from models.movie_predictor import MoviePredictor
    from src.features.store.feature_store import SimpleFeatureStore
    
    store = SimpleFeatureStore()
    
    # 훈련 데이터 로드
    features = store.get_latest_features('processed_movies')
    
    # 모델 초기화 및 훈련
    model = MoviePredictor()
    training_results = model.train(features)
    
    # MLflow에 실험 기록
    import mlflow
    
    with mlflow.start_run():
        mlflow.log_params(model.get_params())
        mlflow.log_metrics(training_results['metrics'])
        mlflow.sklearn.log_model(model, "recommendation_model")
    
    return training_results

def evaluate_model_performance(**context):
    """모델 성능 평가"""
    from models.model_evaluator import ModelEvaluator
    
    evaluator = ModelEvaluator()
    
    # 최신 모델 로드
    latest_model = evaluator.load_latest_model()
    
    # 성능 평가
    evaluation_results = evaluator.evaluate_model(latest_model)
    
    # 성능 기준 확인
    if evaluation_results['accuracy'] < 0.75:
        raise ValueError("모델 성능이 기준에 미달")
    
    return evaluation_results

def deploy_model(**context):
    """모델 배포"""
    from models.model_deployer import ModelDeployer
    
    deployer = ModelDeployer()
    
    # 모델 배포
    deployment_result = deployer.deploy_to_production(
        model_version=context['ds']
    )
    
    return deployment_result

# 태스크 정의
prepare_data = PythonOperator(
    task_id='prepare_data',
    python_callable=prepare_training_data,
    dag=dag,
)

train_model = PythonOperator(
    task_id='train_model',
    python_callable=train_recommendation_model,
    dag=dag,
)

evaluate_model = PythonOperator(
    task_id='evaluate_model',
    python_callable=evaluate_model_performance,
    dag=dag,
)

deploy_model = PythonOperator(
    task_id='deploy_model',
    python_callable=deploy_model,
    dag=dag,
)

# 의존성 설정
prepare_data >> train_model >> evaluate_model >> deploy_model
```

---

## 🔧 에러 처리 및 복구 시스템

### 자동 재시도 및 알림

```python
# src/data_processing/error_handler.py
import logging
from typing import Dict, Any, Optional
from datetime import datetime
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

class PipelineErrorHandler:
    """파이프라인 에러 처리 시스템"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger("PipelineErrorHandler")
        
    def handle_data_collection_error(self, error: Exception, context: Dict[str, Any]):
        """데이터 수집 에러 처리"""
        
        # 에러 로깅
        self.logger.error(f"데이터 수집 실패: {error}")
        
        # 백업 데이터 소스 시도
        if self._should_retry_with_backup(error):
            return self._retry_with_backup_source(context)
        
        # 부분 수집 시도
        if self._should_try_partial_collection(error):
            return self._partial_collection_fallback(context)
        
        # 최종 실패 처리
        self._send_failure_notification(error, context)
        raise error
    
    def handle_feature_processing_error(self, error: Exception, context: Dict[str, Any]):
        """피처 처리 에러 처리"""
        
        # 이전 성공한 피처 사용
        if self._has_previous_features():
            self.logger.warning("이전 피처 사용으로 폴백")
            return self._load_previous_features()
        
        # 기본 피처만 생성
        return self._generate_basic_features_only(context)
    
    def _should_retry_with_backup(self, error: Exception) -> bool:
        """백업 소스 재시도 여부 결정"""
        # API 제한이나 서버 오류인 경우
        return "429" in str(error) or "500" in str(error)
    
    def _send_failure_notification(self, error: Exception, context: Dict[str, Any]):
        """실패 알림 전송"""
        if not self.config.get('email_notifications'):
            return
            
        subject = f"MLOps 파이프라인 실패 알림 - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
        
        body = f"""
        파이프라인 실행이 실패했습니다.
        
        에러: {str(error)}
        시간: {datetime.now().isoformat()}
        컨텍스트: {context}
        
        로그를 확인하여 문제를 해결해주세요.
        """
        
        self._send_email(subject, body)
    
    def _send_email(self, subject: str, body: str):
        """이메일 전송"""
        try:
            msg = MIMEMultipart()
            msg['From'] = self.config['email_from']
            msg['To'] = self.config['email_to']
            msg['Subject'] = subject
            
            msg.attach(MIMEText(body, 'plain'))
            
            server = smtplib.SMTP(self.config['smtp_server'], self.config['smtp_port'])
            server.starttls()
            server.login(self.config['email_from'], self.config['email_password'])
            
            text = msg.as_string()
            server.sendmail(self.config['email_from'], self.config['email_to'], text)
            server.quit()
            
        except Exception as e:
            self.logger.error(f"이메일 전송 실패: {e}")
```

---

## 🔧 데이터 품질 자동 체크

### 실시간 데이터 검증

```python
# src/data_processing/quality_validator.py 확장
class EnhancedQualityValidator:
    """강화된 데이터 품질 검증기"""
    
    def __init__(self):
        self.logger = logging.getLogger("QualityValidator")
        
        # 품질 기준 정의
        self.quality_thresholds = {
            'missing_data_ratio': 0.1,      # 결측값 10% 이하
            'duplicate_ratio': 0.05,        # 중복 5% 이하
            'outlier_ratio': 0.15,          # 이상치 15% 이하
            'schema_compliance': 0.95,      # 스키마 준수율 95% 이상
            'data_freshness_hours': 24,     # 데이터 신선도 24시간 이내
        }
    
    def validate_pipeline_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """파이프라인 데이터 종합 검증"""
        
        validation_results = {
            'timestamp': datetime.now().isoformat(),
            'overall_score': 0.0,
            'passed': False,
            'checks': {}
        }
        
        checks = [
            self._check_data_completeness,
            self._check_data_quality,
            self._check_schema_compliance,
            self._check_data_freshness,
            self._check_business_rules
        ]
        
        total_score = 0
        
        for check in checks:
            result = check(data)
            validation_results['checks'][check.__name__] = result
            total_score += result['score']
        
        validation_results['overall_score'] = total_score / len(checks)
        validation_results['passed'] = validation_results['overall_score'] >= 0.8
        
        # 품질 기준 미달 시 알림
        if not validation_results['passed']:
            self._send_quality_alert(validation_results)
        
        return validation_results
    
    def _check_data_completeness(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """데이터 완전성 검사"""
        
        required_fields = ['id', 'title', 'vote_average', 'popularity', 'release_date']
        missing_counts = {}
        
        for field in required_fields:
            if field in data:
                missing_count = sum(1 for item in data[field] if item is None or item == '')
                missing_counts[field] = missing_count / len(data[field])
        
        avg_missing_ratio = sum(missing_counts.values()) / len(missing_counts)
        
        return {
            'score': 1.0 - avg_missing_ratio,
            'passed': avg_missing_ratio <= self.quality_thresholds['missing_data_ratio'],
            'details': missing_counts
        }
    
    def _check_data_freshness(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """데이터 신선도 검사"""
        
        if 'collection_timestamp' in data:
            collection_time = datetime.fromisoformat(data['collection_timestamp'])
            hours_old = (datetime.now() - collection_time).total_seconds() / 3600
            
            freshness_score = max(0, 1 - (hours_old / self.quality_thresholds['data_freshness_hours']))
            
            return {
                'score': freshness_score,
                'passed': hours_old <= self.quality_thresholds['data_freshness_hours'],
                'hours_old': hours_old
            }
        
        return {'score': 0.5, 'passed': False, 'message': 'No timestamp found'}
    
    def _send_quality_alert(self, validation_results: Dict[str, Any]):
        """품질 알림 전송"""
        self.logger.warning(f"데이터 품질 기준 미달: {validation_results['overall_score']:.2f}")
        
        # Slack 웹훅 전송 (실제 환경에서 구현)
        webhook_url = os.getenv('SLACK_WEBHOOK_URL')
        if webhook_url:
            self._send_slack_notification(webhook_url, validation_results)
    
    def _send_slack_notification(self, webhook_url: str, results: Dict[str, Any]):
        """Slack 알림 전송"""
        import requests
        
        message = {
            "text": f"🚨 데이터 품질 알림",
            "attachments": [
                {
                    "color": "danger",
                    "fields": [
                        {"title": "품질 점수", "value": f"{results['overall_score']:.2f}", "short": True},
                        {"title": "통과 여부", "value": "❌ 실패" if not results['passed'] else "✅ 통과", "short": True}
                    ]
                }
            ]
        }
        
        try:
            requests.post(webhook_url, json=message)
        except Exception as e:
            self.logger.error(f"Slack 알림 전송 실패: {e}")
```

---

## ✅ 완료 기준

### 기능적 완료 기준
- [ ] Airflow DAG 설정 및 실행 확인
- [ ] 자동 스케줄링 동작 검증 (매일/주간)
- [ ] 에러 발생 시 자동 복구 동작 확인
- [ ] 데이터 품질 체크 자동화 동작 검증
- [ ] 알림 시스템 (이메일/Slack) 동작 확인

### 기술적 완료 기준
- [ ] DAG 실행 성공률 95% 이상
- [ ] 에러 복구 성공률 80% 이상
- [ ] 데이터 품질 체크 자동화 100% 적용
- [ ] 파이프라인 실행 시간 2시간 이내
- [ ] 모니터링 대시보드 구축

### 운영 완료 기준
- [ ] 주 7일 무인 운영 가능
- [ ] 장애 발생 시 5분 이내 알림
- [ ] 데이터 품질 SLA 95% 이상 유지
- [ ] 운영 문서 및 가이드 작성 완료
- [ ] 팀원 교육 및 인수인계 완료

---

## 🚀 다음 단계

완료 후 [2.9 모니터링 시스템 구축](./2.9-monitoring-system-guide.md)으로 진행하여 파이프라인 상태를 실시간으로 모니터링하는 시스템을 구축합니다.

---

## 📚 참고 자료

- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [MLOps Pipeline Best Practices](https://ml-ops.org/)
- [Data Quality Monitoring](https://greatexpectations.io/)
- [Pipeline Automation Patterns](https://martinfowler.com/articles/cd4ml.html)
