# 7단계: ML 프레임워크와 모델 서빙 통합 - 상세 구현 가이드

## 📋 단계 개요

**목표**: 다양한 ML 프레임워크와 모델 서빙 플랫폼을 통합하여 안정적인 서비스 제공

**핵심 가치**: 고성능이고 확장 가능한 모델 서빙 시스템으로 실시간 추론 서비스 구축

---

## 🎯 7.1 FastAPI 서버 개발 (추론 API)

### 목표
고성능 REST API를 통한 모델 추론 서비스 구축

### 상세 구현 사항

#### **7.1.1 FastAPI 애플리케이션 구조**
- **메인 애플리케이션 설정**
  ```python
  # src/api/main.py
  from fastapi import FastAPI, HTTPException, Depends
  from fastapi.middleware.cors import CORSMiddleware
  from fastapi.middleware.gzip import GZipMiddleware
  from contextlib import asynccontextmanager
  import logging
  
  from .routers import predictions, health, models
  from .core.config import settings
  from .core.model_loader import ModelLoader
  from .middleware.logging import LoggingMiddleware
  from .middleware.monitoring import MonitoringMiddleware
  
  # 전역 모델 로더
  model_loader = None
  
  @asynccontextmanager
  async def lifespan(app: FastAPI):
      # 시작 시 모델 로딩
      global model_loader
      model_loader = ModelLoader()
      await model_loader.load_models()
      
      yield
      
      # 종료 시 정리
      if model_loader:
          await model_loader.cleanup()
  
  app = FastAPI(
      title="Movie Recommendation API",
      description="ML-powered movie recommendation service",
      version="1.0.0",
      lifespan=lifespan,
      docs_url="/docs" if settings.ENV != "production" else None,
      redoc_url="/redoc" if settings.ENV != "production" else None
  )
  
  # 미들웨어 설정
  app.add_middleware(
      CORSMiddleware,
      allow_origins=settings.ALLOWED_ORIGINS,
      allow_credentials=True,
      allow_methods=["*"],
      allow_headers=["*"],
  )
  app.add_middleware(GZipMiddleware, minimum_size=1000)
  app.add_middleware(LoggingMiddleware)
  app.add_middleware(MonitoringMiddleware)
  
  # 라우터 등록
  app.include_router(health.router, prefix="/health", tags=["health"])
  app.include_router(predictions.router, prefix="/api/v1", tags=["predictions"])
  app.include_router(models.router, prefix="/api/v1/models", tags=["models"])
  
  @app.get("/")
  async def root():
      return {
          "message": "Movie Recommendation API",
          "version": "1.0.0",
          "status": "running"
      }
  ```

#### **7.1.2 예측 API 엔드포인트**
- **추론 라우터 구현**
  ```python
  # src/api/routers/predictions.py
  from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks
  from typing import List, Optional
  import time
  import asyncio
  from uuid import uuid4
  
  from ..schemas.prediction import (
      PredictionRequest, 
      PredictionResponse, 
      BatchPredictionRequest,
      BatchPredictionResponse
  )
  from ..core.dependencies import get_model_loader, get_cache
  from ..core.model_loader import ModelLoader
  from ..core.cache import CacheManager
  from ..core.monitoring import track_prediction
  
  router = APIRouter()
  
  @router.post("/predict", response_model=PredictionResponse)
  @track_prediction
  async def predict_single(
      request: PredictionRequest,
      model_loader: ModelLoader = Depends(get_model_loader),
      cache: CacheManager = Depends(get_cache)
  ):
      \"\"\"단일 사용자 영화 추천\"\"\"
      
      # 캐시 확인
      cache_key = f"predict:{request.user_id}:{hash(str(request.preferences))}"
      cached_result = await cache.get(cache_key)
      if cached_result:
          return PredictionResponse.parse_raw(cached_result)
      
      try:
          # 모델 예측 실행
          start_time = time.time()
          
          predictions = await model_loader.predict(
              user_id=request.user_id,
              preferences=request.preferences,
              top_k=request.top_k or 10
          )
          
          inference_time = time.time() - start_time
          
          # 응답 생성
          response = PredictionResponse(
              user_id=request.user_id,
              recommendations=predictions,
              inference_time_ms=int(inference_time * 1000),
              model_version=model_loader.current_model_version,
              request_id=str(uuid4())
          )
          
          # 캐시 저장 (5분 TTL)
          await cache.set(cache_key, response.json(), ttl=300)
          
          return response
          
      except Exception as e:
          raise HTTPException(
              status_code=500,
              detail=f"Prediction failed: {str(e)}"
          )
  
  @router.post("/batch-predict", response_model=BatchPredictionResponse)
  async def predict_batch(
      request: BatchPredictionRequest,
      background_tasks: BackgroundTasks,
      model_loader: ModelLoader = Depends(get_model_loader)
  ):
      \"\"\"배치 사용자 영화 추천\"\"\"
      
      if len(request.user_ids) > 1000:
          raise HTTPException(
              status_code=400,
              detail="Batch size cannot exceed 1000 users"
          )
      
      try:
          # 배치 예측 실행
          start_time = time.time()
          
          batch_predictions = await model_loader.predict_batch(
              user_ids=request.user_ids,
              top_k=request.top_k or 10
          )
          
          total_time = time.time() - start_time
          
          response = BatchPredictionResponse(
              predictions=batch_predictions,
              total_users=len(request.user_ids),
              total_time_ms=int(total_time * 1000),
              average_time_per_user_ms=int((total_time / len(request.user_ids)) * 1000),
              model_version=model_loader.current_model_version,
              batch_id=str(uuid4())
          )
          
          # 비동기로 사용 통계 업데이트
          background_tasks.add_task(
              update_usage_statistics,
              batch_size=len(request.user_ids),
              inference_time=total_time
          )
          
          return response
          
      except Exception as e:
          raise HTTPException(
              status_code=500,
              detail=f"Batch prediction failed: {str(e)}"
          )
  
  @router.get("/recommendations/{user_id}")
  async def get_user_recommendations(
      user_id: int,
      top_k: Optional[int] = 10,
      model_loader: ModelLoader = Depends(get_model_loader)
  ):
      \"\"\"특정 사용자 추천 조회\"\"\"
      
      try:
          recommendations = await model_loader.get_recommendations(
              user_id=user_id,
              top_k=top_k
          )
          
          return {
              "user_id": user_id,
              "recommendations": recommendations,
              "count": len(recommendations)
          }
          
      except Exception as e:
          raise HTTPException(
              status_code=500,
              detail=f"Failed to get recommendations: {str(e)}"
          )
  
  async def update_usage_statistics(batch_size: int, inference_time: float):
      \"\"\"사용 통계 업데이트 (백그라운드 태스크)\"\"\"
      # 통계 업데이트 로직
      pass
  ```

#### **7.1.3 API 스키마 정의**
- **Pydantic 모델 스키마**
  ```python
  # src/api/schemas/prediction.py
  from pydantic import BaseModel, Field, validator
  from typing import List, Optional, Dict, Any
  from datetime import datetime
  
  class UserPreferences(BaseModel):
      \"\"\"사용자 선호도\"\"\"
      favorite_genres: Optional[List[str]] = None
      min_rating: Optional[float] = Field(None, ge=0, le=10)
      preferred_languages: Optional[List[str]] = None
      exclude_adult: Optional[bool] = True
      preferred_decade: Optional[str] = None
  
  class PredictionRequest(BaseModel):
      \"\"\"단일 예측 요청\"\"\"
      user_id: int = Field(..., description="사용자 ID")
      preferences: Optional[UserPreferences] = None
      top_k: Optional[int] = Field(10, ge=1, le=50, description="추천 영화 수")
      include_reasons: Optional[bool] = Field(False, description="추천 이유 포함 여부")
      
      @validator('user_id')
      def validate_user_id(cls, v):
          if v <= 0:
              raise ValueError('User ID must be positive')
          return v
  
  class MovieRecommendation(BaseModel):
      \"\"\"영화 추천 결과\"\"\"
      movie_id: int
      title: str
      score: float = Field(..., ge=0, le=1)
      genres: List[str]
      release_year: Optional[int] = None
      rating: Optional[float] = None
      poster_url: Optional[str] = None
      reason: Optional[str] = None
  
  class PredictionResponse(BaseModel):
      \"\"\"단일 예측 응답\"\"\"
      user_id: int
      recommendations: List[MovieRecommendation]
      inference_time_ms: int
      model_version: str
      request_id: str
      timestamp: datetime = Field(default_factory=datetime.now)
  
  class BatchPredictionRequest(BaseModel):
      \"\"\"배치 예측 요청\"\"\"
      user_ids: List[int] = Field(..., min_items=1, max_items=1000)
      top_k: Optional[int] = Field(10, ge=1, le=50)
      
      @validator('user_ids')
      def validate_user_ids(cls, v):
          if len(set(v)) != len(v):
              raise ValueError('Duplicate user IDs not allowed')
          if any(uid <= 0 for uid in v):
              raise ValueError('All user IDs must be positive')
          return v
  
  class UserPredictions(BaseModel):
      \"\"\"사용자별 예측 결과\"\"\"
      user_id: int
      recommendations: List[MovieRecommendation]
      success: bool = True
      error_message: Optional[str] = None
  
  class BatchPredictionResponse(BaseModel):
      \"\"\"배치 예측 응답\"\"\"
      predictions: List[UserPredictions]
      total_users: int
      successful_predictions: int = 0
      failed_predictions: int = 0
      total_time_ms: int
      average_time_per_user_ms: int
      model_version: str
      batch_id: str
      timestamp: datetime = Field(default_factory=datetime.now)
      
      @validator('successful_predictions', always=True)
      def calculate_successful(cls, v, values):
          if 'predictions' in values:
              return sum(1 for p in values['predictions'] if p.success)
          return v
      
      @validator('failed_predictions', always=True)
      def calculate_failed(cls, v, values):
          if 'predictions' in values:
              return sum(1 for p in values['predictions'] if not p.success)
          return v
  ```

---

## 🎯 7.2 모델 로딩 및 추론 로직 구현

### 목표
효율적이고 확장 가능한 모델 로딩 및 추론 시스템 구축

### 상세 구현 사항

#### **7.2.1 모델 로더 클래스**
- **비동기 모델 관리**
  ```python
  # src/api/core/model_loader.py
  import asyncio
  import pickle
  import numpy as np
  from typing import Dict, List, Optional, Any
  import mlflow.pyfunc
  from mlflow.tracking import MlflowClient
  import logging
  
  from .config import settings
  from ..schemas.prediction import MovieRecommendation, UserPreferences
  
  class ModelLoader:
      def __init__(self):
          self.client = MlflowClient(settings.MLFLOW_TRACKING_URI)
          self.current_model = None
          self.current_model_version = None
          self.model_metadata = {}
          self.movie_catalog = {}
          self.user_embeddings = {}
          self._lock = asyncio.Lock()
          self.logger = logging.getLogger(__name__)
      
      async def load_models(self):
          \"\"\"최신 프로덕션 모델 로드\"\"\"
          async with self._lock:
              try:
                  # 프로덕션 모델 조회
                  latest_version = self.client.get_latest_versions(
                      name=settings.MODEL_NAME,
                      stages=["Production"]
                  )
                  
                  if not latest_version:
                      raise Exception("No production model found")
                  
                  model_version = latest_version[0]
                  model_uri = f"models:/{settings.MODEL_NAME}/{model_version.version}"
                  
                  # 모델 로드
                  self.current_model = mlflow.pyfunc.load_model(model_uri)
                  self.current_model_version = model_version.version
                  
                  # 메타데이터 로드
                  await self._load_model_metadata(model_version)
                  
                  # 영화 카탈로그 로드
                  await self._load_movie_catalog()
                  
                  self.logger.info(f"Successfully loaded model version {self.current_model_version}")
                  
              except Exception as e:
                  self.logger.error(f"Failed to load model: {e}")
                  raise
      
      async def _load_model_metadata(self, model_version):
          \"\"\"모델 메타데이터 로드\"\"\"
          run_id = model_version.run_id
          run = self.client.get_run(run_id)
          
          self.model_metadata = {
              'accuracy': run.data.metrics.get('accuracy', 0),
              'precision': run.data.metrics.get('precision', 0),
              'recall': run.data.metrics.get('recall', 0),
              'training_date': run.info.start_time,
              'hyperparameters': run.data.params
          }
      
      async def _load_movie_catalog(self):
          \"\"\"영화 카탈로그 데이터 로드\"\"\"
          # 실제 구현에서는 데이터베이스나 캐시에서 로드
          self.movie_catalog = {
              # movie_id: {title, genres, rating, poster_url, ...}
          }
      
      async def predict(
          self, 
          user_id: int, 
          preferences: Optional[UserPreferences] = None,
          top_k: int = 10
      ) -> List[MovieRecommendation]:
          \"\"\"단일 사용자 예측\"\"\"
          
          if not self.current_model:
              raise Exception("Model not loaded")
          
          try:
              # 입력 데이터 준비
              input_data = await self._prepare_input_data(user_id, preferences)
              
              # 모델 예측
              predictions = self.current_model.predict(input_data)
              
              # 결과 후처리
              recommendations = await self._postprocess_predictions(
                  predictions, user_id, top_k, preferences
              )
              
              return recommendations
              
          except Exception as e:
              self.logger.error(f"Prediction failed for user {user_id}: {e}")
              raise
      
      async def predict_batch(
          self, 
          user_ids: List[int], 
          top_k: int = 10
      ) -> List[Dict[str, Any]]:
          \"\"\"배치 사용자 예측\"\"\"
          
          if not self.current_model:
              raise Exception("Model not loaded")
          
          batch_results = []
          
          # 배치를 청크로 나누어 처리
          chunk_size = 100
          for i in range(0, len(user_ids), chunk_size):
              chunk = user_ids[i:i + chunk_size]
              
              try:
                  # 청크별 예측
                  chunk_results = await self._predict_chunk(chunk, top_k)
                  batch_results.extend(chunk_results)
                  
              except Exception as e:
                  self.logger.error(f"Batch prediction failed for chunk {i//chunk_size}: {e}")
                  # 실패한 사용자들을 오류로 표시
                  for user_id in chunk:
                      batch_results.append({
                          "user_id": user_id,
                          "recommendations": [],
                          "success": False,
                          "error_message": str(e)
                      })
          
          return batch_results
      
      async def _predict_chunk(self, user_ids: List[int], top_k: int):
          \"\"\"청크 단위 예측\"\"\"
          results = []
          
          for user_id in user_ids:
              try:
                  recommendations = await self.predict(user_id, top_k=top_k)
                  results.append({
                      "user_id": user_id,
                      "recommendations": recommendations,
                      "success": True
                  })
              except Exception as e:
                  results.append({
                      "user_id": user_id,
                      "recommendations": [],
                      "success": False,
                      "error_message": str(e)
                  })
          
          return results
      
      async def _prepare_input_data(self, user_id: int, preferences: Optional[UserPreferences]):
          \"\"\"모델 입력 데이터 준비\"\"\"
          # 사용자 피처 생성
          user_features = await self._get_user_features(user_id)
          
          # 선호도 피처 추가
          if preferences:
              preference_features = await self._encode_preferences(preferences)
              user_features.update(preference_features)
          
          # 모델 입력 형태로 변환
          input_array = np.array([list(user_features.values())])
          
          return input_array
      
      async def _get_user_features(self, user_id: int) -> Dict[str, float]:
          \"\"\"사용자 피처 조회\"\"\"
          # 실제 구현에서는 피처 스토어에서 조회
          return {
              'user_id': float(user_id),
              'avg_rating': 7.5,
              'total_ratings': 50,
              'preferred_genre_action': 0.3,
              'preferred_genre_drama': 0.7
          }
      
      async def _encode_preferences(self, preferences: UserPreferences) -> Dict[str, float]:
          \"\"\"사용자 선호도 인코딩\"\"\"
          encoded = {}
          
          if preferences.favorite_genres:
              for genre in preferences.favorite_genres:
                  encoded[f'pref_genre_{genre.lower()}'] = 1.0
          
          if preferences.min_rating:
              encoded['min_rating'] = preferences.min_rating
          
          return encoded
      
      async def _postprocess_predictions(
          self, 
          predictions: np.ndarray, 
          user_id: int, 
          top_k: int,
          preferences: Optional[UserPreferences]
      ) -> List[MovieRecommendation]:
          \"\"\"예측 결과 후처리\"\"\"
          
          # 상위 k개 영화 선택
          top_indices = np.argsort(predictions[0])[-top_k:][::-1]
          top_scores = predictions[0][top_indices]
          
          recommendations = []
          
          for idx, (movie_idx, score) in enumerate(zip(top_indices, top_scores)):
              movie_info = self.movie_catalog.get(movie_idx, {})
              
              recommendation = MovieRecommendation(
                  movie_id=movie_idx,
                  title=movie_info.get('title', f'Movie {movie_idx}'),
                  score=float(score),
                  genres=movie_info.get('genres', []),
                  release_year=movie_info.get('release_year'),
                  rating=movie_info.get('rating'),
                  poster_url=movie_info.get('poster_url'),
                  reason=await self._generate_recommendation_reason(
                      movie_info, preferences, score
                  )
              )
              
              recommendations.append(recommendation)
          
          return recommendations
      
      async def _generate_recommendation_reason(
          self, 
          movie_info: Dict, 
          preferences: Optional[UserPreferences],
          score: float
      ) -> Optional[str]:
          \"\"\"추천 이유 생성\"\"\"
          if not preferences:
              return None
          
          reasons = []
          
          if preferences.favorite_genres:
              common_genres = set(movie_info.get('genres', [])) & set(preferences.favorite_genres)
              if common_genres:
                  reasons.append(f"선호 장르 ({', '.join(common_genres)}) 포함")
          
          if preferences.min_rating and movie_info.get('rating', 0) >= preferences.min_rating:
              reasons.append(f"높은 평점 ({movie_info['rating']:.1f})")
          
          if score > 0.8:
              reasons.append("높은 추천 점수")
          
          return "; ".join(reasons) if reasons else None
      
      async def reload_model(self):
          \"\"\"모델 재로드 (핫 스와핑)\"\"\"
          old_version = self.current_model_version
          
          try:
              await self.load_models()
              self.logger.info(f"Model reloaded from version {old_version} to {self.current_model_version}")
              
          except Exception as e:
              self.logger.error(f"Failed to reload model: {e}")
              raise
      
      async def get_model_info(self) -> Dict[str, Any]:
          \"\"\"모델 정보 조회\"\"\"
          return {
              'model_version': self.current_model_version,
              'metadata': self.model_metadata,
              'catalog_size': len(self.movie_catalog),
              'status': 'loaded' if self.current_model else 'not_loaded'
          }
      
      async def cleanup(self):
          \"\"\"리소스 정리\"\"\"
          self.current_model = None
          self.current_model_version = None
          self.model_metadata = {}
          self.movie_catalog = {}
          self.logger.info("Model loader cleaned up")
  ```

#### **7.2.2 캐싱 시스템**
- **Redis 기반 캐시 매니저**
  ```python
  # src/api/core/cache.py
  import redis.asyncio as redis
  import json
  import pickle
  from typing import Any, Optional
  import logging
  
  from .config import settings
  
  class CacheManager:
      def __init__(self):
          self.redis_client = None
          self.logger = logging.getLogger(__name__)
      
      async def connect(self):
          \"\"\"Redis 연결\"\"\"
          try:
              self.redis_client = redis.Redis(
                  host=settings.REDIS_HOST,
                  port=settings.REDIS_PORT,
                  password=settings.REDIS_PASSWORD,
                  decode_responses=True,
                  max_connections=20
              )
              
              # 연결 테스트
              await self.redis_client.ping()
              self.logger.info("Connected to Redis cache")
              
          except Exception as e:
              self.logger.error(f"Failed to connect to Redis: {e}")
              raise
      
      async def get(self, key: str) -> Optional[str]:
          \"\"\"캐시에서 값 조회\"\"\"
          if not self.redis_client:
              return None
          
          try:
              value = await self.redis_client.get(key)
              if value:
                  self.logger.debug(f"Cache hit for key: {key}")
              return value
              
          except Exception as e:
              self.logger.error(f"Cache get error for key {key}: {e}")
              return None
      
      async def set(self, key: str, value: str, ttl: int = 3600):
          \"\"\"캐시에 값 저장\"\"\"
          if not self.redis_client:
              return False
          
          try:
              await self.redis_client.setex(key, ttl, value)
              self.logger.debug(f"Cache set for key: {key}, TTL: {ttl}")
              return True
              
          except Exception as e:
              self.logger.error(f"Cache set error for key {key}: {e}")
              return False
      
      async def delete(self, key: str):
          \"\"\"캐시에서 키 삭제\"\"\"
          if not self.redis_client:
              return False
          
          try:
              result = await self.redis_client.delete(key)
              return result > 0
              
          except Exception as e:
              self.logger.error(f"Cache delete error for key {key}: {e}")
              return False
      
      async def get_json(self, key: str) -> Optional[dict]:
          \"\"\"JSON 형태로 캐시 조회\"\"\"
          value = await self.get(key)
          if value:
              try:
                  return json.loads(value)
              except json.JSONDecodeError:
                  self.logger.error(f"Failed to decode JSON for key: {key}")
          return None
      
      async def set_json(self, key: str, value: dict, ttl: int = 3600):
          \"\"\"JSON 형태로 캐시 저장\"\"\"
          try:
              json_value = json.dumps(value)
              return await self.set(key, json_value, ttl)
          except Exception as e:
              self.logger.error(f"Failed to encode JSON for key {key}: {e}")
              return False
      
      async def close(self):
          \"\"\"Redis 연결 종료\"\"\"
          if self.redis_client:
              await self.redis_client.close()
              self.logger.info("Redis connection closed")
  ```

이 7단계를 완료하면 웹 인터페이스를 통해 고성능 모델 예측 서비스를 제공하고, 다수의 동시 요청을 안정적으로 처리할 수 있는 견고한 모델 서빙 시스템이 구축됩니다.
