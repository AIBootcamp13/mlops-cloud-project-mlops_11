# 6단계: 모델 레지스트리 중심의 모델 관리 체계 - 상세 구현 가이드

## 📋 단계 개요

**목표**: 체계적인 모델 생명주기 관리로 모델 품질과 거버넌스 확보

**핵심 가치**: 모델 레지스트리가 워크플로우의 중심에 위치하여 모든 학습된 모델이 레지스트리를 거쳐 체계적으로 관리되는 시스템 구축

---

## 🎯 6.1 MLflow Tracking Server 설치 및 설정

### 목표
실험 추적과 모델 관리를 위한 중앙화된 MLflow 서버 구축

### 상세 구현 사항

#### **6.1.1 MLflow 서버 설치**
- **Docker Compose를 활용한 MLflow 환경 구축**
  ```yaml
  # docker-compose.mlflow.yml
  version: '3.8'
  
  services:
    postgres:
      image: postgres:15
      container_name: mlflow-postgres
      environment:
        POSTGRES_DB: mlflow
        POSTGRES_USER: mlflow
        POSTGRES_PASSWORD: mlflow_password
      volumes:
        - postgres_data:/var/lib/postgresql/data
      ports:
        - "5432:5432"
      restart: always
  
    minio:
      image: minio/minio:latest
      container_name: mlflow-minio
      environment:
        MINIO_ACCESS_KEY: minio_access_key
        MINIO_SECRET_KEY: minio_secret_key
      volumes:
        - minio_data:/data
      ports:
        - "9000:9000"
        - "9001:9001"
      command: server /data --console-address ":9001"
      restart: always
  
    mlflow:
      image: python:3.11-slim
      container_name: mlflow-server
      depends_on:
        - postgres
        - minio
      environment:
        MLFLOW_BACKEND_STORE_URI: postgresql://mlflow:mlflow_password@postgres:5432/mlflow
        MLFLOW_DEFAULT_ARTIFACT_ROOT: s3://mlflow-artifacts/
        AWS_ACCESS_KEY_ID: minio_access_key
        AWS_SECRET_ACCESS_KEY: minio_secret_key
        MLFLOW_S3_ENDPOINT_URL: http://minio:9000
      ports:
        - "5000:5000"
      volumes:
        - ./mlflow-entrypoint.sh:/entrypoint.sh
      command: /entrypoint.sh
      restart: always
  
  volumes:
    postgres_data:
    minio_data:
  ```

- **MLflow 서버 실행 스크립트**
  ```bash
  #!/bin/bash
  # mlflow-entrypoint.sh
  
  # MLflow 설치
  pip install mlflow[extras]==2.22.0 psycopg2-binary boto3
  
  # MinIO 버킷 생성을 위한 대기
  sleep 10
  
  # MLflow 서버 시작
  mlflow server \
    --backend-store-uri $MLFLOW_BACKEND_STORE_URI \
    --default-artifact-root $MLFLOW_DEFAULT_ARTIFACT_ROOT \
    --host 0.0.0.0 \
    --port 5000
  ```

#### **6.1.2 MLflow 클라이언트 설정**
- **Python 클라이언트 설정**
  ```python
  # src/mlflow_config.py
  import mlflow
  import os
  from mlflow.tracking import MlflowClient
  
  class MLflowConfig:
      \"\"\"MLflow 설정 관리\"\"\"
      
      def __init__(self):
          # MLflow 서버 URI 설정
          self.tracking_uri = os.getenv("MLFLOW_TRACKING_URI", "http://localhost:5000")
          self.experiment_name = os.getenv("MLFLOW_EXPERIMENT_NAME", "movie_recommendation")
          
          # MLflow 설정
          mlflow.set_tracking_uri(self.tracking_uri)
          
          # 실험 설정
          self._setup_experiment()
          
          # 클라이언트 초기화
          self.client = MlflowClient(tracking_uri=self.tracking_uri)
      
      def _setup_experiment(self):
          \"\"\"실험 설정\"\"\"
          try:
              # 실험이 존재하는지 확인
              experiment = mlflow.get_experiment_by_name(self.experiment_name)
              if experiment is None:
                  # 실험 생성
                  experiment_id = mlflow.create_experiment(
                      name=self.experiment_name,
                      tags={
                          "project": "mlops-movie-recommendation",
                          "version": "1.0",
                          "description": "Movie recommendation system experiments"
                      }
                  )
                  print(f"실험 생성됨: {self.experiment_name} (ID: {experiment_id})")
              else:
                  experiment_id = experiment.experiment_id
                  print(f"기존 실험 사용: {self.experiment_name} (ID: {experiment_id})")
              
              # 활성 실험 설정
              mlflow.set_experiment(self.experiment_name)
              
          except Exception as e:
              print(f"실험 설정 실패: {e}")
              raise
      
      def get_client(self):
          \"\"\"MLflow 클라이언트 반환\"\"\"
          return self.client
  ```

---

## 🎯 6.2 실험 추적 시스템 구축

### 목표
모든 ML 실험을 체계적으로 추적하고 비교할 수 있는 시스템 구현

### 상세 구현 사항

#### **6.2.1 실험 추적 래퍼 클래스**
- **MLflow 실험 추적 통합**
  ```python
  # src/tracking/experiment_tracker.py
  import mlflow
  import mlflow.sklearn
  import mlflow.pytorch
  import numpy as np
  import pickle
  import json
  from datetime import datetime
  from typing import Dict, Any, Optional, Union
  import logging
  
  class ExperimentTracker:
      \"\"\"MLflow 실험 추적기\"\"\"
      
      def __init__(self, experiment_name: str = "movie_recommendation"):
          self.experiment_name = experiment_name
          self.logger = logging.getLogger(__name__)
          
          # MLflow 설정
          mlflow.set_experiment(experiment_name)
          
      def start_run(self, run_name: Optional[str] = None, 
                   tags: Optional[Dict[str, str]] = None):
          \"\"\"실험 실행 시작\"\"\"
          
          # 기본 태그 설정
          default_tags = {
              "timestamp": datetime.now().isoformat(),
              "project": "mlops-movie-recommendation",
              "stage": "development"
          }
          
          if tags:
              default_tags.update(tags)
              
          # MLflow 실행 시작
          mlflow.start_run(run_name=run_name, tags=default_tags)
          
          run_id = mlflow.active_run().info.run_id
          self.logger.info(f"실험 실행 시작: {run_id}")
          
          return run_id
          
      def log_parameters(self, params: Dict[str, Any]):
          \"\"\"하이퍼파라미터 로깅\"\"\"
          try:
              # 단순 타입으로 변환
              clean_params = {}
              for key, value in params.items():
                  if isinstance(value, (int, float, str, bool)):
                      clean_params[key] = value
                  else:
                      clean_params[key] = str(value)
              
              mlflow.log_params(clean_params)
              self.logger.info(f"파라미터 로깅 완료: {len(clean_params)}개")
              
          except Exception as e:
              self.logger.error(f"파라미터 로깅 실패: {e}")
              
      def log_metrics(self, metrics: Dict[str, Union[int, float]], 
                     step: Optional[int] = None):
          \"\"\"메트릭 로깅\"\"\"
          try:
              for metric_name, metric_value in metrics.items():
                  if isinstance(metric_value, (int, float, np.integer, np.floating)):
                      mlflow.log_metric(metric_name, float(metric_value), step=step)
                  
              self.logger.info(f"메트릭 로깅 완료: {len(metrics)}개")
              
          except Exception as e:
              self.logger.error(f"메트릭 로깅 실패: {e}")
              
      def log_model(self, model, model_name: str, 
                   model_type: str = "sklearn",
                   signature: Optional[Any] = None,
                   input_example: Optional[Any] = None):
          \"\"\"모델 로깅\"\"\"
          try:
              if model_type == "sklearn":
                  mlflow.sklearn.log_model(
                      sk_model=model,
                      artifact_path=model_name,
                      signature=signature,
                      input_example=input_example
                  )
              elif model_type == "pytorch":
                  mlflow.pytorch.log_model(
                      pytorch_model=model,
                      artifact_path=model_name,
                      signature=signature,
                      input_example=input_example
                  )
              elif model_type == "custom":
                  # 커스텀 모델 (numpy 기반 등)
                  self._log_custom_model(model, model_name)
              
              self.logger.info(f"모델 로깅 완료: {model_name}")
              
          except Exception as e:
              self.logger.error(f"모델 로깅 실패: {e}")
              
      def _log_custom_model(self, model, model_name: str):
          \"\"\"커스텀 모델 로깅\"\"\"
          # 모델을 pickle로 직렬화
          model_bytes = pickle.dumps(model)
          
          # 임시 파일로 저장 후 아티팩트로 로깅
          import tempfile
          with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as f:
              f.write(model_bytes)
              f.flush()
              mlflow.log_artifact(f.name, artifact_path=f"{model_name}/model.pkl")
              
      def log_artifacts(self, artifacts: Dict[str, Any]):
          \"\"\"아티팩트 로깅\"\"\"
          try:
              import tempfile
              import os
              
              for artifact_name, artifact_data in artifacts.items():
                  if isinstance(artifact_data, dict):
                      # JSON으로 저장
                      with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                          json.dump(artifact_data, f, indent=2, default=str)
                          f.flush()
                          mlflow.log_artifact(f.name, artifact_path=f"artifacts/{artifact_name}.json")
                          os.unlink(f.name)
                          
                  elif isinstance(artifact_data, str):
                      # 텍스트로 저장
                      with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
                          f.write(artifact_data)
                          f.flush()
                          mlflow.log_artifact(f.name, artifact_path=f"artifacts/{artifact_name}.txt")
                          os.unlink(f.name)
                          
              self.logger.info(f"아티팩트 로깅 완료: {len(artifacts)}개")
              
          except Exception as e:
              self.logger.error(f"아티팩트 로깅 실패: {e}")
              
      def log_dataset_info(self, dataset_info: Dict[str, Any]):
          \"\"\"데이터셋 정보 로깅\"\"\"
          try:
              # 데이터셋 메타데이터를 파라미터로 로깅
              dataset_params = {}
              for key, value in dataset_info.items():
                  if key.startswith('dataset_'):
                      dataset_params[key] = str(value)
                  else:
                      dataset_params[f'dataset_{key}'] = str(value)
              
              mlflow.log_params(dataset_params)
              
              # 데이터셋 통계를 메트릭으로 로깅
              if 'train_size' in dataset_info:
                  mlflow.log_metric('dataset_train_size', dataset_info['train_size'])
              if 'test_size' in dataset_info:
                  mlflow.log_metric('dataset_test_size', dataset_info['test_size'])
              if 'feature_count' in dataset_info:
                  mlflow.log_metric('dataset_feature_count', dataset_info['feature_count'])
                  
              self.logger.info("데이터셋 정보 로깅 완료")
              
          except Exception as e:
              self.logger.error(f"데이터셋 정보 로깅 실패: {e}")
              
      def end_run(self, status: str = "FINISHED"):
          \"\"\"실험 실행 종료\"\"\"
          try:
              if mlflow.active_run():
                  run_id = mlflow.active_run().info.run_id
                  mlflow.end_run(status=status)
                  self.logger.info(f"실험 실행 종료: {run_id}")
              
          except Exception as e:
              self.logger.error(f"실험 종료 실패: {e}")
  ```

#### **6.2.2 모델 훈련과 추적 통합**
- **훈련 과정 추적**
  ```python
  # src/training/tracked_trainer.py
  import mlflow
  from typing import Dict, Any, Optional
  import numpy as np
  from sklearn.metrics import accuracy_score, precision_recall_fscore_support
  
  from ..tracking.experiment_tracker import ExperimentTracker
  
  class TrackedModelTrainer:
      \"\"\"MLflow 추적이 통합된 모델 훈련기\"\"\"
      
      def __init__(self, experiment_name: str = "movie_recommendation"):
          self.tracker = ExperimentTracker(experiment_name)
          
      def train_with_tracking(self, model_class, train_data, val_data, 
                            hyperparameters: Dict[str, Any],
                            run_name: Optional[str] = None):
          \"\"\"추적과 함께 모델 훈련\"\"\"
          
          # 실험 실행 시작
          run_id = self.tracker.start_run(
              run_name=run_name,
              tags={
                  "model_type": model_class.__name__,
                  "training_stage": "supervised_learning"
              }
          )
          
          try:
              # 하이퍼파라미터 로깅
              self.tracker.log_parameters(hyperparameters)
              
              # 데이터셋 정보 로깅
              dataset_info = {
                  "train_size": len(train_data),
                  "val_size": len(val_data),
                  "feature_count": train_data.shape[1] if hasattr(train_data, 'shape') else 'unknown',
                  "training_date": mlflow.utils.time.get_current_time_millis()
              }
              self.tracker.log_dataset_info(dataset_info)
              
              # 모델 초기화
              model = model_class(**hyperparameters)
              
              # 훈련 실행
              training_history = self._train_model(model, train_data, val_data)
              
              # 훈련 히스토리 로깅
              for epoch, metrics in enumerate(training_history):
                  self.tracker.log_metrics(metrics, step=epoch)
              
              # 최종 모델 평가
              final_metrics = self._evaluate_model(model, val_data)
              self.tracker.log_metrics(final_metrics)
              
              # 모델 저장
              self.tracker.log_model(
                  model=model,
                  model_name="movie_recommender",
                  model_type="custom"
              )
              
              # 성공적으로 완료
              self.tracker.end_run(status="FINISHED")
              
              return {
                  "run_id": run_id,
                  "model": model,
                  "metrics": final_metrics,
                  "training_history": training_history
              }
              
          except Exception as e:
              # 실패 시 상태 업데이트
              self.tracker.end_run(status="FAILED")
              raise
              
      def _train_model(self, model, train_data, val_data):
          \"\"\"실제 모델 훈련 로직\"\"\"
          training_history = []
          
          # 에포크별 훈련 (예시)
          for epoch in range(10):
              # 훈련 스텝
              train_loss = model.train_step(train_data)
              
              # 검증 스텝
              val_loss, val_accuracy = model.validate(val_data)
              
              # 메트릭 기록
              epoch_metrics = {
                  "train_loss": float(train_loss),
                  "val_loss": float(val_loss),
                  "val_accuracy": float(val_accuracy)
              }
              
              training_history.append(epoch_metrics)
              
          return training_history
          
      def _evaluate_model(self, model, test_data):
          \"\"\"모델 평가\"\"\"
          # 예측 수행
          predictions = model.predict(test_data)
          
          # 실제 라벨 (예시)
          y_true = test_data['labels']  # 실제 구현에 맞게 수정
          
          # 메트릭 계산
          accuracy = accuracy_score(y_true, predictions)
          precision, recall, f1, _ = precision_recall_fscore_support(
              y_true, predictions, average='weighted'
          )
          
          return {
              "accuracy": float(accuracy),
              "precision": float(precision),
              "recall": float(recall),
              "f1_score": float(f1)
          }
  ```

---

## 🎯 6.3 모델 레지스트리 설정

### 목표
중앙화된 모델 저장소로 모델 버전 관리 및 배포 프로세스 구축

### 상세 구현 사항

#### **6.3.1 모델 레지스트리 관리자**
- **모델 등록 및 관리**
  ```python
  # src/registry/model_registry_manager.py
  import mlflow
  from mlflow.tracking import MlflowClient
  from mlflow.entities.model_registry import RegisteredModel, ModelVersion
  from typing import Dict, Any, List, Optional
  import logging
  from datetime import datetime
  
  class ModelRegistryManager:
      \"\"\"MLflow 모델 레지스트리 관리자\"\"\"
      
      def __init__(self, tracking_uri: str = "http://localhost:5000"):
          self.client = MlflowClient(tracking_uri=tracking_uri)
          self.logger = logging.getLogger(__name__)
          
      def register_model(self, model_name: str, run_id: str, 
                        description: Optional[str] = None,
                        tags: Optional[Dict[str, str]] = None) -> str:
          \"\"\"모델 레지스트리에 모델 등록\"\"\"
          try:
              # 모델 아티팩트 URI 생성
              model_uri = f"runs:/{run_id}/movie_recommender"
              
              # 모델 등록
              model_version = mlflow.register_model(
                  model_uri=model_uri,
                  name=model_name
              )
              
              # 설명 추가
              if description:
                  self.client.update_model_version(
                      name=model_name,
                      version=model_version.version,
                      description=description
                  )
              
              # 태그 추가
              if tags:
                  for key, value in tags.items():
                      self.client.set_model_version_tag(
                          name=model_name,
                          version=model_version.version,
                          key=key,
                          value=value
                      )
              
              self.logger.info(
                  f"모델 등록 완료: {model_name} v{model_version.version}"
              )
              
              return model_version.version
              
          except Exception as e:
              self.logger.error(f"모델 등록 실패: {e}")
              raise
              
      def get_model_versions(self, model_name: str) -> List[ModelVersion]:
          \"\"\"모델의 모든 버전 조회\"\"\"
          try:
              model_versions = self.client.search_model_versions(
                  filter_string=f"name='{model_name}'"
              )
              
              # 버전 번호로 정렬 (최신 순)
              model_versions.sort(
                  key=lambda x: int(x.version), 
                  reverse=True
              )
              
              return model_versions
              
          except Exception as e:
              self.logger.error(f"모델 버전 조회 실패: {e}")
              return []
              
      def get_latest_model_version(self, model_name: str, 
                                 stage: str = "Production") -> Optional[ModelVersion]:
          \"\"\"특정 스테이지의 최신 모델 버전 조회\"\"\"
          try:
              latest_versions = self.client.get_latest_versions(
                  name=model_name,
                  stages=[stage]
              )
              
              if latest_versions:
                  return latest_versions[0]
              else:
                  self.logger.warning(f"스테이지 '{stage}'에 모델이 없음: {model_name}")
                  return None
                  
          except Exception as e:
              self.logger.error(f"최신 모델 버전 조회 실패: {e}")
              return None
              
      def transition_model_stage(self, model_name: str, version: str, 
                               stage: str, archive_existing: bool = True) -> bool:
          \"\"\"모델 스테이지 전환\"\"\"
          try:
              # 기존 프로덕션 모델을 아카이브로 이동 (선택사항)
              if stage == "Production" and archive_existing:
                  current_prod = self.get_latest_model_version(model_name, "Production")
                  if current_prod:
                      self.client.transition_model_version_stage(
                          name=model_name,
                          version=current_prod.version,
                          stage="Archived"
                      )
                      self.logger.info(
                          f"기존 프로덕션 모델 아카이브: v{current_prod.version}"
                      )
              
              # 새 모델을 해당 스테이지로 전환
              self.client.transition_model_version_stage(
                  name=model_name,
                  version=version,
                  stage=stage
              )
              
              self.logger.info(
                  f"모델 스테이지 전환 완료: {model_name} v{version} -> {stage}"
              )
              
              return True
              
          except Exception as e:
              self.logger.error(f"모델 스테이지 전환 실패: {e}")
              return False
              
      def compare_model_versions(self, model_name: str, 
                               version1: str, version2: str) -> Dict[str, Any]:
          \"\"\"두 모델 버전 비교\"\"\"
          try:
              # 모델 버전 정보 조회
              mv1 = self.client.get_model_version(model_name, version1)
              mv2 = self.client.get_model_version(model_name, version2)
              
              # 실행 정보 조회
              run1 = self.client.get_run(mv1.run_id)
              run2 = self.client.get_run(mv2.run_id)
              
              comparison = {
                  "model_name": model_name,
                  "version1": {
                      "version": version1,
                      "stage": mv1.current_stage,
                      "metrics": run1.data.metrics,
                      "params": run1.data.params,
                      "creation_time": mv1.creation_timestamp
                  },
                  "version2": {
                      "version": version2,
                      "stage": mv2.current_stage,
                      "metrics": run2.data.metrics,
                      "params": run2.data.params,
                      "creation_time": mv2.creation_timestamp
                  }
              }
              
              # 성능 차이 계산
              if 'accuracy' in run1.data.metrics and 'accuracy' in run2.data.metrics:
                  acc_diff = run2.data.metrics['accuracy'] - run1.data.metrics['accuracy']
                  comparison['performance_diff'] = {
                      'accuracy_improvement': acc_diff,
                      'better_model': version2 if acc_diff > 0 else version1
                  }
              
              return comparison
              
          except Exception as e:
              self.logger.error(f"모델 버전 비교 실패: {e}")
              return {}
              
      def delete_model_version(self, model_name: str, version: str) -> bool:
          \"\"\"모델 버전 삭제\"\"\"
          try:
              self.client.delete_model_version(
                  name=model_name,
                  version=version
              )
              
              self.logger.info(f"모델 버전 삭제 완료: {model_name} v{version}")
              return True
              
          except Exception as e:
              self.logger.error(f"모델 버전 삭제 실패: {e}")
              return False
              
      def get_model_lineage(self, model_name: str, version: str) -> Dict[str, Any]:
          \"\"\"모델 계보 정보 조회\"\"\"
          try:
              model_version = self.client.get_model_version(model_name, version)
              run = self.client.get_run(model_version.run_id)
              
              lineage = {
                  "model_name": model_name,
                  "version": version,
                  "run_id": model_version.run_id,
                  "experiment_id": run.info.experiment_id,
                  "creation_time": model_version.creation_timestamp,
                  "source": model_version.source,
                  "current_stage": model_version.current_stage,
                  "tags": model_version.tags,
                  "metrics": run.data.metrics,
                  "parameters": run.data.params
              }
              
              # 데이터셋 정보 추출
              dataset_params = {
                  k: v for k, v in run.data.params.items() 
                  if k.startswith('dataset_')
              }
              if dataset_params:
                  lineage['dataset_info'] = dataset_params
              
              return lineage
              
          except Exception as e:
              self.logger.error(f"모델 계보 조회 실패: {e}")
              return {}
  ```

---

## 🎯 6.4 모델 승인 프로세스 구축

### 목표
프로덕션 배포 전 모델 품질 검증 및 승인 워크플로우 구현

### 상세 구현 사항

#### **6.4.1 모델 검증 시스템**
- **자동 모델 검증**
  ```python
  # src/validation/model_validator.py
  import mlflow
  from mlflow.tracking import MlflowClient
  from typing import Dict, Any, List, Tuple, Optional
  import numpy as np
  import logging
  from datetime import datetime, timedelta
  
  class ModelValidator:
      \"\"\"모델 검증 시스템\"\"\"
      
      def __init__(self, tracking_uri: str = "http://localhost:5000"):
          self.client = MlflowClient(tracking_uri=tracking_uri)
          self.logger = logging.getLogger(__name__)
          
          # 검증 기준 설정
          self.validation_criteria = {
              "min_accuracy": 0.75,
              "min_precision": 0.70,
              "min_recall": 0.65,
              "max_inference_time_ms": 100,
              "min_data_quality_score": 0.8
          }
          
      def validate_model_version(self, model_name: str, version: str) -> Dict[str, Any]:
          \"\"\"모델 버전 검증\"\"\"
          try:
              # 모델 버전 정보 조회
              model_version = self.client.get_model_version(model_name, version)
              run = self.client.get_run(model_version.run_id)
              
              validation_results = {
                  "model_name": model_name,
                  "version": version,
                  "validation_timestamp": datetime.now().isoformat(),
                  "passed": True,
                  "validation_details": {},
                  "recommendations": []
              }
              
              # 1. 성능 메트릭 검증
              performance_check = self._validate_performance_metrics(run.data.metrics)
              validation_results["validation_details"]["performance"] = performance_check
              
              if not performance_check["passed"]:
                  validation_results["passed"] = False
              
              # 2. 데이터 품질 검증
              data_quality_check = self._validate_data_quality(run.data.params)
              validation_results["validation_details"]["data_quality"] = data_quality_check
              
              if not data_quality_check["passed"]:
                  validation_results["passed"] = False
              
              # 3. 모델 크기 및 복잡도 검증
              complexity_check = self._validate_model_complexity(run.data.params)
              validation_results["validation_details"]["complexity"] = complexity_check
              
              # 4. 이전 모델과의 비교
              comparison_check = self._compare_with_baseline(model_name, run.data.metrics)
              validation_results["validation_details"]["baseline_comparison"] = comparison_check
              
              if not comparison_check["passed"]:
                  validation_results["passed"] = False
              
              # 검증 결과를 모델 버전에 태그로 추가
              self._add_validation_tags(model_name, version, validation_results)
              
              return validation_results
              
          except Exception as e:
              self.logger.error(f"모델 검증 실패: {e}")
              return {
                  "passed": False,
                  "error": str(e),
                  "validation_timestamp": datetime.now().isoformat()
              }
              
      def _validate_performance_metrics(self, metrics: Dict[str, float]) -> Dict[str, Any]:
          \"\"\"성능 메트릭 검증\"\"\"
          check_results = {
              "passed": True,
              "details": {},
              "failed_criteria": []
          }
          
          # 정확도 검증
          if "accuracy" in metrics:
              accuracy = metrics["accuracy"]
              check_results["details"]["accuracy"] = {
                  "value": accuracy,
                  "threshold": self.validation_criteria["min_accuracy"],
                  "passed": accuracy >= self.validation_criteria["min_accuracy"]
              }
              
              if accuracy < self.validation_criteria["min_accuracy"]:
                  check_results["passed"] = False
                  check_results["failed_criteria"].append("accuracy")
          
          # 정밀도 검증
          if "precision" in metrics:
              precision = metrics["precision"]
              check_results["details"]["precision"] = {
                  "value": precision,
                  "threshold": self.validation_criteria["min_precision"],
                  "passed": precision >= self.validation_criteria["min_precision"]
              }
              
              if precision < self.validation_criteria["min_precision"]:
                  check_results["passed"] = False
                  check_results["failed_criteria"].append("precision")
          
          # 재현율 검증
          if "recall" in metrics:
              recall = metrics["recall"]
              check_results["details"]["recall"] = {
                  "value": recall,
                  "threshold": self.validation_criteria["min_recall"],
                  "passed": recall >= self.validation_criteria["min_recall"]
              }
              
              if recall < self.validation_criteria["min_recall"]:
                  check_results["passed"] = False
                  check_results["failed_criteria"].append("recall")
          
          return check_results
          
      def _validate_data_quality(self, params: Dict[str, str]) -> Dict[str, Any]:
          \"\"\"데이터 품질 검증\"\"\"
          check_results = {
              "passed": True,
              "details": {},
              "warnings": []
          }
          
          # 데이터셋 크기 확인
          if "dataset_train_size" in params:
              train_size = int(params["dataset_train_size"])
              check_results["details"]["train_size"] = train_size
              
              if train_size < 1000:
                  check_results["warnings"].append("Training dataset is small (<1000 samples)")
          
          # 피처 수 확인
          if "dataset_feature_count" in params:
              feature_count = int(params["dataset_feature_count"])
              check_results["details"]["feature_count"] = feature_count
              
              if feature_count < 3:
                  check_results["warnings"].append("Low number of features")
          
          return check_results
          
      def _validate_model_complexity(self, params: Dict[str, str]) -> Dict[str, Any]:
          \"\"\"모델 복잡도 검증\"\"\"
          check_results = {
              "passed": True,
              "details": {},
              "warnings": []
          }
          
          # 하이퍼파라미터 검증
          if "hidden_size" in params:
              hidden_size = int(params["hidden_size"])
              check_results["details"]["hidden_size"] = hidden_size
              
              if hidden_size > 1000:
                  check_results["warnings"].append("Large hidden layer size may cause overfitting")
          
          if "learning_rate" in params:
              lr = float(params["learning_rate"])
              check_results["details"]["learning_rate"] = lr
              
              if lr > 0.1:
                  check_results["warnings"].append("High learning rate may cause instability")
              elif lr < 0.0001:
                  check_results["warnings"].append("Very low learning rate may slow convergence")
          
          return check_results
          
      def _compare_with_baseline(self, model_name: str, 
                                current_metrics: Dict[str, float]) -> Dict[str, Any]:
          \"\"\"기준 모델과 비교\"\"\"
          check_results = {
              "passed": True,
              "details": {},
              "improvement": {}
          }
          
          try:
              # 현재 프로덕션 모델 조회
              prod_versions = self.client.get_latest_versions(
                  name=model_name,
                  stages=["Production"]
              )
              
              if not prod_versions:
                  check_results["details"]["baseline"] = "No production model to compare"
                  return check_results
              
              # 기준 모델 메트릭 조회
              baseline_run = self.client.get_run(prod_versions[0].run_id)
              baseline_metrics = baseline_run.data.metrics
              
              # 성능 비교
              if "accuracy" in current_metrics and "accuracy" in baseline_metrics:
                  current_acc = current_metrics["accuracy"]
                  baseline_acc = baseline_metrics["accuracy"]
                  improvement = current_acc - baseline_acc
                  
                  check_results["improvement"]["accuracy"] = {
                      "current": current_acc,
                      "baseline": baseline_acc,
                      "improvement": improvement,
                      "improvement_percent": (improvement / baseline_acc) * 100
                  }
                  
                  # 성능이 현저히 떨어지는 경우 실패
                  if improvement < -0.05:  # 5% 이상 성능 저하
                      check_results["passed"] = False
                      check_results["details"]["failure_reason"] = "Significant performance degradation"
              
              return check_results
              
          except Exception as e:
              check_results["details"]["comparison_error"] = str(e)
              return check_results
              
      def _add_validation_tags(self, model_name: str, version: str, 
                             validation_results: Dict[str, Any]):
          \"\"\"검증 결과를 모델 태그에 추가\"\"\"
          try:
              # 검증 상태 태그
              self.client.set_model_version_tag(
                  name=model_name,
                  version=version,
                  key="validation_status",
                  value="passed" if validation_results["passed"] else "failed"
              )
              
              # 검증 시간 태그
              self.client.set_model_version_tag(
                  name=model_name,
                  version=version,
                  key="validation_timestamp",
                  value=validation_results["validation_timestamp"]
              )
              
              # 성능 메트릭 태그
              if "performance" in validation_results["validation_details"]:
                  perf = validation_results["validation_details"]["performance"]
                  if "details" in perf and "accuracy" in perf["details"]:
                      acc_info = perf["details"]["accuracy"]
                      self.client.set_model_version_tag(
                          name=model_name,
                          version=version,
                          key="validation_accuracy",
                          value=str(acc_info["value"])
                      )
              
          except Exception as e:
              self.logger.error(f"검증 태그 추가 실패: {e}")
  ```

#### **6.4.2 승인 워크플로우**
- **자동 승인 시스템**
  ```python
  # src/approval/approval_workflow.py
  import mlflow
  from mlflow.tracking import MlflowClient
  from typing import Dict, Any, List, Optional
  import logging
  from datetime import datetime
  from enum import Enum
  
  from ..validation.model_validator import ModelValidator
  from ..registry.model_registry_manager import ModelRegistryManager
  
  class ApprovalStatus(Enum):
      PENDING = "pending"
      APPROVED = "approved"
      REJECTED = "rejected"
      NEEDS_REVIEW = "needs_review"
  
  class ModelApprovalWorkflow:
      \"\"\"모델 승인 워크플로우\"\"\"
      
      def __init__(self, tracking_uri: str = "http://localhost:5000"):
          self.client = MlflowClient(tracking_uri=tracking_uri)
          self.validator = ModelValidator(tracking_uri)
          self.registry_manager = ModelRegistryManager(tracking_uri)
          self.logger = logging.getLogger(__name__)
          
          # 자동 승인 기준
          self.auto_approval_criteria = {
              "min_accuracy_improvement": 0.02,  # 2% 이상 개선
              "max_performance_degradation": 0.01,  # 1% 이하 성능 저하
              "required_validations": ["performance", "data_quality"]
          }
          
      def submit_for_approval(self, model_name: str, version: str,
                            submitted_by: str = "system",
                            notes: Optional[str] = None) -> Dict[str, Any]:
          \"\"\"승인 요청 제출\"\"\"
          try:
              # 1. 모델 검증 실행
              validation_results = self.validator.validate_model_version(model_name, version)
              
              # 2. 승인 상태 결정
              approval_decision = self._determine_approval_status(validation_results)
              
              # 3. 승인 메타데이터 생성
              approval_metadata = {
                  "model_name": model_name,
                  "version": version,
                  "submitted_by": submitted_by,
                  "submission_timestamp": datetime.now().isoformat(),
                  "status": approval_decision["status"].value,
                  "validation_results": validation_results,
                  "approval_reason": approval_decision["reason"],
                  "notes": notes
              }
              
              # 4. 승인 정보를 모델 태그에 저장
              self._save_approval_metadata(model_name, version, approval_metadata)
              
              # 5. 자동 승인된 경우 스테이지 전환
              if approval_decision["status"] == ApprovalStatus.APPROVED:
                  self._auto_promote_model(model_name, version)
              
              self.logger.info(
                  f"승인 요청 처리 완료: {model_name} v{version} - {approval_decision['status'].value}"
              )
              
              return approval_metadata
              
          except Exception as e:
              self.logger.error(f"승인 요청 처리 실패: {e}")
              raise
              
      def _determine_approval_status(self, validation_results: Dict[str, Any]) -> Dict[str, Any]:
          \"\"\"승인 상태 결정\"\"\"
          
          # 검증 실패 시 거부
          if not validation_results.get("passed", False):
              return {
                  "status": ApprovalStatus.REJECTED,
                  "reason": "Model failed validation checks"
              }
          
          # 성능 개선 확인
          baseline_comparison = validation_results.get("validation_details", {}).get("baseline_comparison", {})
          
          if "improvement" in baseline_comparison and "accuracy" in baseline_comparison["improvement"]:
              accuracy_improvement = baseline_comparison["improvement"]["accuracy"]["improvement"]
              
              # 상당한 성능 개선 시 자동 승인
              if accuracy_improvement >= self.auto_approval_criteria["min_accuracy_improvement"]:
                  return {
                      "status": ApprovalStatus.APPROVED,
                      "reason": f"Significant accuracy improvement: {accuracy_improvement:.3f}"
                  }
              
              # 성능 저하가 임계값 내인 경우 승인
              elif accuracy_improvement >= -self.auto_approval_criteria["max_performance_degradation"]:
                  return {
                      "status": ApprovalStatus.APPROVED,
                      "reason": "Performance maintained within acceptable range"
                  }
              
              # 성능 저하가 심한 경우 검토 필요
              else:
                  return {
                      "status": ApprovalStatus.NEEDS_REVIEW,
                      "reason": f"Performance degradation requires review: {accuracy_improvement:.3f}"
                  }
          
          # 기준 모델이 없는 경우 (첫 번째 모델)
          else:
              performance = validation_results.get("validation_details", {}).get("performance", {})
              if performance.get("passed", False):
                  return {
                      "status": ApprovalStatus.APPROVED,
                      "reason": "First model meets validation criteria"
                  }
              else:
                  return {
                      "status": ApprovalStatus.NEEDS_REVIEW,
                      "reason": "First model requires manual review"
                  }
              
      def _save_approval_metadata(self, model_name: str, version: str, 
                                 approval_metadata: Dict[str, Any]):
          \"\"\"승인 메타데이터 저장\"\"\"
          try:
              # 주요 승인 정보를 태그로 저장
              tags_to_set = {
                  "approval_status": approval_metadata["status"],
                  "approval_timestamp": approval_metadata["submission_timestamp"],
                  "approval_reason": approval_metadata["approval_reason"],
                  "submitted_by": approval_metadata["submitted_by"]
              }
              
              for key, value in tags_to_set.items():
                  self.client.set_model_version_tag(
                      name=model_name,
                      version=version,
                      key=key,
                      value=str(value)
                  )
              
              # 상세 승인 정보를 아티팩트로 저장
              import json
              import tempfile
              
              with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                  json.dump(approval_metadata, f, indent=2, default=str)
                  f.flush()
                  
                  # 모델 버전의 run_id 조회
                  model_version = self.client.get_model_version(model_name, version)
                  
                  # 아티팩트 로깅 (새로운 run에서)
                  with mlflow.start_run(run_id=model_version.run_id):
                      mlflow.log_artifact(f.name, artifact_path="approval/approval_metadata.json")
              
          except Exception as e:
              self.logger.error(f"승인 메타데이터 저장 실패: {e}")
              
      def _auto_promote_model(self, model_name: str, version: str):
          \"\"\"자동 모델 승급\"\"\"
          try:
              # 모델을 Staging으로 전환
              success = self.registry_manager.transition_model_stage(
                  model_name=model_name,
                  version=version,
                  stage="Staging",
                  archive_existing=True
              )
              
              if success:
                  # 승급 태그 추가
                  self.client.set_model_version_tag(
                      name=model_name,
                      version=version,
                      key="auto_promoted",
                      value="true"
                  )
                  
                  self.client.set_model_version_tag(
                      name=model_name,
                      version=version,
                      key="promotion_timestamp",
                      value=datetime.now().isoformat()
                  )
                  
                  self.logger.info(f"자동 승급 완료: {model_name} v{version} -> Staging")
              
          except Exception as e:
              self.logger.error(f"자동 승급 실패: {e}")
              
      def get_pending_approvals(self) -> List[Dict[str, Any]]:
          \"\"\"승인 대기 중인 모델 조회\"\"\"
          try:
              # 모든 등록된 모델 조회
              registered_models = self.client.search_registered_models()
              
              pending_approvals = []
              
              for model in registered_models:
                  # 각 모델의 버전들 조회
                  versions = self.client.search_model_versions(
                      filter_string=f"name='{model.name}'"
                  )
                  
                  for version in versions:
                      # 승인 상태 확인
                      approval_status = version.tags.get("approval_status", "unknown")
                      
                      if approval_status in ["pending", "needs_review"]:
                          pending_approvals.append({
                              "model_name": model.name,
                              "version": version.version,
                              "status": approval_status,
                              "submission_time": version.tags.get("approval_timestamp", "unknown"),
                              "current_stage": version.current_stage
                          })
              
              return pending_approvals
              
          except Exception as e:
              self.logger.error(f"승인 대기 모델 조회 실패: {e}")
              return []
              
      def manual_approve(self, model_name: str, version: str, 
                        approved_by: str, notes: Optional[str] = None) -> bool:
          \"\"\"수동 승인\"\"\"
          try:
              # 승인 태그 업데이트
              self.client.set_model_version_tag(
                  name=model_name,
                  version=version,
                  key="approval_status",
                  value="approved"
              )
              
              self.client.set_model_version_tag(
                  name=model_name,
                  version=version,
                  key="approved_by",
                  value=approved_by
              )
              
              self.client.set_model_version_tag(
                  name=model_name,
                  version=version,
                  key="manual_approval_timestamp",
                  value=datetime.now().isoformat()
              )
              
              if notes:
                  self.client.set_model_version_tag(
                      name=model_name,
                      version=version,
                      key="approval_notes",
                      value=notes
                  )
              
              # Staging으로 승급
              self._auto_promote_model(model_name, version)
              
              self.logger.info(f"수동 승인 완료: {model_name} v{version} by {approved_by}")
              return True
              
          except Exception as e:
              self.logger.error(f"수동 승인 실패: {e}")
              return False
  ```

---

## ✅ 완료 기준

### 6.4.1 기능적 완료 기준
- [ ] 모든 실험과 모델이 MLflow에서 중앙 관리됨
- [ ] 모델 버전별로 성능과 메타데이터가 자동 추적됨
- [ ] 모델 검증 시스템이 품질 기준에 따라 자동 검증 수행
- [ ] 승인 프로세스를 통해 검증된 모델만 프로덕션 배포
- [ ] 모델 비교 및 계보 추적이 가능함

### 6.4.2 기술적 완료 기준
- [ ] MLflow Tracking Server 안정 운영
- [ ] PostgreSQL 백엔드 스토어 구성 완료
- [ ] MinIO 아티팩트 스토어 연동 완료
- [ ] 자동 모델 검증 및 승인 워크플로우 구현
- [ ] 모델 레지스트리 API 통합 완료

### 6.4.3 운영적 완료 기준
- [ ] 모델 등록부터 배포까지 전체 프로세스 자동화
- [ ] 모델 성능 저하 시 자동 감지 및 대응
- [ ] 모델 거버넌스 정책 준수 확인
- [ ] 모델 감사 추적(audit trail) 완전 구현
- [ ] 팀원 대상 모델 레지스트리 사용 교육 완료

---

## 🚀 다음 단계 준비

### 7단계 연계 작업
- 모델 레지스트리에서 승인된 모델을 7단계 모델 서빙 시스템으로 자동 배포
- 서빙 성능 메트릭을 모델 레지스트리로 피드백
- A/B 테스트 결과를 모델 버전 비교에 활용

### 고도화 계획
- MLflow Projects를 활용한 재현 가능한 실험 환경
- 모델 성능 자동 모니터링 및 재훈련 트리거
- 다중 환경(dev/staging/prod) 모델 레지스트리 구성
- 모델 배포 자동화 및 카나리 배포 지원

이 6단계를 완료하면 모든 모델이 체계적으로 관리되고 검증된 모델만 프로덕션에 배포되는 견고한 모델 거버넌스 체계가 구축됩니다.
