"""Airflow DAG with MLflow integration for movie recommendation pipeline"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.utils.dates import days_ago
import logging
import os
import sys

# í”„ë¡œì íŠ¸ ê²½ë¡œë¥¼ Python pathì— ì¶”ê°€
sys.path.append('/app')
sys.path.append('/app/src')

logger = logging.getLogger(__name__)

# ê¸°ë³¸ DAG ì„¤ì •
default_args = {
    'owner': 'movie-mlops',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# DAG ì •ì˜
dag = DAG(
    'mlflow_integration_pipeline',
    default_args=default_args,
    description='Movie recommendation pipeline with MLflow integration',
    schedule_interval=timedelta(days=1),  # ë§¤ì¼ ì‹¤í–‰
    catchup=False,
    tags=['mlflow', 'pytorch', 'movie-recommendation'],
)


def setup_mlflow_experiment(**context):
    """MLflow ì‹¤í—˜ ì„¤ì •"""
    try:
        from src.mlflow.experiment_tracker import create_experiment_tracker
        
        # ì‹¤í—˜ ì¶”ì ê¸° ìƒì„±
        experiment_name = "movie_recommendation_pipeline"
        tracker = create_experiment_tracker(experiment_name)
        
        logger.info(f"MLflow ì‹¤í—˜ ì„¤ì • ì™„ë£Œ: {experiment_name}")
        
        # ì‹¤í—˜ IDë¥¼ XComì— ì €ì¥í•˜ì—¬ ë‹¤ë¥¸ íƒœìŠ¤í¬ì—ì„œ ì‚¬ìš©
        context['task_instance'].xcom_push(
            key='experiment_name', 
            value=experiment_name
        )
        
        return experiment_name
        
    except Exception as e:
        logger.error(f"MLflow ì‹¤í—˜ ì„¤ì • ì‹¤íŒ¨: {e}")
        raise


def train_pytorch_model_with_mlflow(**context):
    """PyTorch ëª¨ë¸ í›ˆë ¨ ë° MLflow ë¡œê¹…"""
    try:
        from src.mlflow.experiment_tracker import create_experiment_tracker, MLflowRunContext
        from src.models.pytorch.training import train_pytorch_model
        import torch
        
        # ì‹¤í—˜ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
        experiment_name = context['task_instance'].xcom_pull(
            task_ids='setup_mlflow_experiment',
            key='experiment_name'
        )
        
        if not experiment_name:
            experiment_name = "movie_recommendation_pipeline"
        
        # ì‹¤í—˜ ì¶”ì ê¸° ìƒì„±
        tracker = create_experiment_tracker(experiment_name)
        
        # í›ˆë ¨ ì„¤ì •
        training_config = {
            'batch_size': 256,
            'num_epochs': 20,
            'learning_rate': 0.001,
            'optimizer_type': 'adam',
            'early_stopping_patience': 5
        }
        
        model_config = {
            'embedding_dim': 64,
            'hidden_dims': [128, 64, 32],
            'dropout_rate': 0.3
        }
        
        # MLflow ì‹¤í–‰ ì‹œì‘
        run_name = f"pytorch_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        tags = {
            'model_type': 'pytorch',
            'pipeline': 'airflow',
            'environment': 'production'
        }
        
        with MLflowRunContext(tracker, run_name, tags) as mlf:
            # í›ˆë ¨ íŒŒë¼ë¯¸í„° ë¡œê¹…
            all_params = {**training_config, **model_config}
            mlf.log_params(all_params)
            
            # ì‹œìŠ¤í…œ ì •ë³´ ë¡œê¹…
            system_info = {
                'pytorch_version': torch.__version__,
                'cuda_available': torch.cuda.is_available(),
                'device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0
            }
            mlf.log_params(system_info)
            
            # ëª¨ë¸ í›ˆë ¨
            model, history = train_pytorch_model(
                data_path="data/processed/watch_log.csv",
                model_type="neural_cf",
                model_config=model_config,
                training_config=training_config,
                experiment_name=f"airflow_{experiment_name}"
            )
            
            # í›ˆë ¨ ê¸°ë¡ ë¡œê¹…
            mlf.log_training_history(history)
            
            # ëª¨ë¸ ë¡œê¹…
            mlf.log_model(
                model=model,
                model_name="pytorch_movie_recommender",
                model_type="pytorch"
            )
            
            # ìµœì¢… ì„±ëŠ¥ ë©”íŠ¸ë¦­
            final_metrics = {
                'final_train_loss': history['train_loss'][-1],
                'best_train_loss': min(history['train_loss']),
                'total_epochs': len(history['train_loss'])
            }
            
            if history['val_loss']:
                final_metrics.update({
                    'final_val_loss': history['val_loss'][-1],
                    'best_val_loss': min(history['val_loss'])
                })
            
            mlf.log_metrics(final_metrics)
            
            # ëª¨ë¸ URIë¥¼ XComì— ì €ì¥
            run_id = mlf._tracker.client.get_run(mlf._tracker.client.get_experiment_by_name(experiment_name).experiment_id).info.run_id
            model_uri = f"runs:/{run_id}/pytorch_movie_recommender"
            
            context['task_instance'].xcom_push(
                key='model_uri',
                value=model_uri
            )
            context['task_instance'].xcom_push(
                key='run_id',
                value=run_id
            )
            
            logger.info(f"PyTorch ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ: {model_uri}")
            
            return {
                'model_uri': model_uri,
                'run_id': run_id,
                'final_metrics': final_metrics
            }
        
    except Exception as e:
        logger.error(f"PyTorch ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {e}")
        raise


def register_model_to_registry(**context):
    """í›ˆë ¨ëœ ëª¨ë¸ì„ MLflow ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡"""
    try:
        from src.mlflow.model_registry import get_model_registry
        
        # ëª¨ë¸ URI ê°€ì ¸ì˜¤ê¸°
        model_uri = context['task_instance'].xcom_pull(
            task_ids='train_pytorch_model',
            key='model_uri'
        )
        
        run_id = context['task_instance'].xcom_pull(
            task_ids='train_pytorch_model',
            key='run_id'
        )
        
        if not model_uri:
            raise ValueError("ëª¨ë¸ URIë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ìƒì„±
        registry = get_model_registry()
        
        # ëª¨ë¸ ë“±ë¡
        model_name = "movie_recommender_pytorch"
        description = f"PyTorch ì˜í™” ì¶”ì²œ ëª¨ë¸ - Airflow íŒŒì´í”„ë¼ì¸ ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')})"
        
        tags = {
            'pipeline': 'airflow',
            'model_framework': 'pytorch',
            'training_date': datetime.now().strftime('%Y-%m-%d'),
            'run_id': run_id
        }
        
        model_version_info = registry.register_model(
            model_uri=model_uri,
            model_name=model_name,
            description=description,
            tags=tags
        )
        
        logger.info(f"ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë“±ë¡ ì™„ë£Œ: {model_name} v{model_version_info['version']}")
        
        # ëª¨ë¸ ë²„ì „ ì •ë³´ë¥¼ XComì— ì €ì¥
        context['task_instance'].xcom_push(
            key='registered_model_name',
            value=model_name
        )
        context['task_instance'].xcom_push(
            key='registered_model_version',
            value=model_version_info['version']
        )
        
        return model_version_info
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë“±ë¡ ì‹¤íŒ¨: {e}")
        raise


def evaluate_model_performance(**context):
    """ë“±ë¡ëœ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
    try:
        from src.mlflow.model_registry import get_model_registry
        from src.models.pytorch.data_loader import prepare_data_from_watch_log, create_data_loaders
        from src.models.pytorch.inference import MovieRecommenderInference
        import torch
        import numpy as np
        from sklearn.metrics import mean_squared_error, mean_absolute_error
        
        # ë“±ë¡ëœ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        model_name = context['task_instance'].xcom_pull(
            task_ids='register_model',
            key='registered_model_name'
        )
        
        model_version = context['task_instance'].xcom_pull(
            task_ids='register_model', 
            key='registered_model_version'
        )
        
        if not model_name or not model_version:
            raise ValueError("ë“±ë¡ëœ ëª¨ë¸ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ ëª¨ë¸ ë¡œë“œ
        registry = get_model_registry()
        model = registry.load_model(model_name, version=model_version)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
        train_df, val_df, test_df = prepare_data_from_watch_log(
            "data/processed/watch_log.csv"
        )
        
        data_loaders = create_data_loaders(
            train_df=train_df,
            test_df=test_df,
            batch_size=256
        )
        
        # ëª¨ë¸ í‰ê°€
        model.eval()
        predictions = []
        actuals = []
        
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model.to(device)
        
        with torch.no_grad():
            for batch in data_loaders['test']:
                user_ids = batch['user_id'].to(device)
                movie_ids = batch['movie_id'].to(device)
                ratings = batch['rating'].to(device)
                
                outputs = model(user_ids, movie_ids)
                
                predictions.extend(outputs.cpu().numpy())
                actuals.extend(ratings.cpu().numpy())
        
        predictions = np.array(predictions)
        actuals = np.array(actuals)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        rmse = np.sqrt(mean_squared_error(actuals, predictions))
        mae = mean_absolute_error(actuals, predictions)
        mse = mean_squared_error(actuals, predictions)
        
        evaluation_metrics = {
            'test_rmse': rmse,
            'test_mae': mae,
            'test_mse': mse,
            'test_samples': len(predictions)
        }
        
        # MLflowì— í‰ê°€ ê²°ê³¼ ë¡œê¹…
        from src.mlflow.experiment_tracker import create_experiment_tracker
        
        experiment_name = context['task_instance'].xcom_pull(
            task_ids='setup_mlflow_experiment',
            key='experiment_name'
        )
        
        tracker = create_experiment_tracker(experiment_name)
        
        # í‰ê°€ ì‹¤í–‰ ì‹œì‘
        run_name = f"model_evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        tags = {
            'stage': 'evaluation',
            'model_name': model_name,
            'model_version': str(model_version)
        }
        
        with tracker.start_run(run_name, tags):
            tracker.log_metrics(evaluation_metrics)
            tracker.log_predictions(predictions, actuals)
        
        logger.info(f"ëª¨ë¸ í‰ê°€ ì™„ë£Œ - RMSE: {rmse:.4f}, MAE: {mae:.4f}")
        
        # ì„±ëŠ¥ ê¸°ì¤€ í™•ì¸ (ì˜ˆ: RMSE < 1.0)
        performance_threshold = 1.0
        
        context['task_instance'].xcom_push(
            key='evaluation_metrics',
            value=evaluation_metrics
        )
        context['task_instance'].xcom_push(
            key='performance_passed',
            value=rmse < performance_threshold
        )
        
        return evaluation_metrics
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ í‰ê°€ ì‹¤íŒ¨: {e}")
        raise


def decide_model_promotion(**context):
    """ëª¨ë¸ í”„ë¡œë•ì…˜ ìŠ¹ê²© ì—¬ë¶€ ê²°ì •"""
    try:
        from src.mlflow.model_registry import get_model_registry
        
        # í‰ê°€ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        performance_passed = context['task_instance'].xcom_pull(
            task_ids='evaluate_model',
            key='performance_passed'
        )
        
        evaluation_metrics = context['task_instance'].xcom_pull(
            task_ids='evaluate_model',
            key='evaluation_metrics'
        )
        
        model_name = context['task_instance'].xcom_pull(
            task_ids='register_model',
            key='registered_model_name'
        )
        
        model_version = context['task_instance'].xcom_pull(
            task_ids='register_model',
            key='registered_model_version'
        )
        
        registry = get_model_registry()
        
        if performance_passed:
            # ì„±ëŠ¥ ê¸°ì¤€ì„ í†µê³¼í•œ ê²½ìš° Stagingìœ¼ë¡œ ìŠ¹ê²©
            success = registry.transition_model_stage(
                model_name=model_name,
                version=model_version,
                stage='Staging',
                description=f"ìë™ ìŠ¹ê²© - ì„±ëŠ¥ ê¸°ì¤€ í†µê³¼ (RMSE: {evaluation_metrics['test_rmse']:.4f})"
            )
            
            if success:
                logger.info(f"ëª¨ë¸ Staging ìŠ¹ê²© ì™„ë£Œ: {model_name} v{model_version}")
                
                # ì¶”ê°€ ê²€ì¦ ë¡œì§ì´ ìˆë‹¤ë©´ ì—¬ê¸°ì— ì¶”ê°€
                # ì˜ˆ: A/B í…ŒìŠ¤íŠ¸, ì¶”ê°€ ê²€ì¦ ë“±
                
                # ëª¨ë“  ê²€ì¦ì„ í†µê³¼í•˜ë©´ Productionìœ¼ë¡œ ìŠ¹ê²©
                prod_success = registry.promote_model_to_production(
                    model_name=model_name,
                    version=model_version,
                    description="ìë™ í”„ë¡œë•ì…˜ ìŠ¹ê²© - ëª¨ë“  ê²€ì¦ í†µê³¼"
                )
                
                if prod_success:
                    logger.info(f"ëª¨ë¸ Production ìŠ¹ê²© ì™„ë£Œ: {model_name} v{model_version}")
                    promotion_status = "production"
                else:
                    logger.warning(f"ëª¨ë¸ Production ìŠ¹ê²© ì‹¤íŒ¨: {model_name} v{model_version}")
                    promotion_status = "staging"
            else:
                logger.warning(f"ëª¨ë¸ Staging ìŠ¹ê²© ì‹¤íŒ¨: {model_name} v{model_version}")
                promotion_status = "none"
        else:
            logger.info(f"ëª¨ë¸ ì„±ëŠ¥ ê¸°ì¤€ ë¯¸ë‹¬: {model_name} v{model_version}")
            promotion_status = "none"
        
        context['task_instance'].xcom_push(
            key='promotion_status',
            value=promotion_status
        )
        
        return {
            'model_name': model_name,
            'model_version': model_version,
            'promotion_status': promotion_status,
            'performance_passed': performance_passed
        }
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ìŠ¹ê²© ê²°ì • ì‹¤íŒ¨: {e}")
        raise


def cleanup_old_models(**context):
    """ì˜¤ë˜ëœ ëª¨ë¸ ë²„ì „ ì •ë¦¬"""
    try:
        from src.mlflow.model_registry import get_model_registry
        
        registry = get_model_registry()
        
        # ë“±ë¡ëœ ëª¨ë“  ëª¨ë¸ ì¡°íšŒ
        models = registry.list_models()
        
        for model in models:
            model_name = model['name']
            
            # ê° ëª¨ë¸ì˜ ëª¨ë“  ë²„ì „ ì¡°íšŒ
            versions = registry.client.search_model_versions(f"name='{model_name}'")
            
            # ë²„ì „ì„ ìƒì„± ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìµœì‹  -> ì˜¤ë˜ëœ ìˆœ)
            versions_sorted = sorted(
                versions, 
                key=lambda x: x.creation_timestamp, 
                reverse=True
            )
            
            # Production, Staging ë‹¨ê³„ê°€ ì•„ë‹Œ ì˜¤ë˜ëœ ë²„ì „ë“¤ (5ê°œ ì´ˆê³¼) ì•„ì¹´ì´ë¸Œ
            keep_count = 5
            archived_count = 0
            
            for i, version in enumerate(versions_sorted):
                if (i >= keep_count and 
                    version.current_stage in ['None'] and
                    version.creation_timestamp < (datetime.now() - timedelta(days=7)).timestamp() * 1000):
                    
                    # ì•„ì¹´ì´ë¸Œë¡œ ì´ë™
                    registry.transition_model_stage(
                        model_name=model_name,
                        version=version.version,
                        stage='Archived',
                        description="ìë™ ì•„ì¹´ì´ë¸Œ - ì˜¤ë˜ëœ ë²„ì „"
                    )
                    archived_count += 1
            
            if archived_count > 0:
                logger.info(f"ëª¨ë¸ {model_name}: {archived_count}ê°œ ë²„ì „ ì•„ì¹´ì´ë¸Œ")
        
        logger.info("ì˜¤ë˜ëœ ëª¨ë¸ ì •ë¦¬ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        # ì •ë¦¬ ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•Šìœ¼ë¯€ë¡œ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚¤ì§€ ì•ŠìŒ
        pass


def send_notification(**context):
    """íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ì•Œë¦¼"""
    try:
        promotion_status = context['task_instance'].xcom_pull(
            task_ids='decide_promotion',
            key='promotion_status'
        )
        
        evaluation_metrics = context['task_instance'].xcom_pull(
            task_ids='evaluate_model',
            key='evaluation_metrics'
        )
        
        model_name = context['task_instance'].xcom_pull(
            task_ids='register_model',
            key='registered_model_name'
        )
        
        model_version = context['task_instance'].xcom_pull(
            task_ids='register_model',
            key='registered_model_version'
        )
        
        # ê°„ë‹¨í•œ ë¡œê·¸ ê¸°ë°˜ ì•Œë¦¼ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ìŠ¬ë™, ì´ë©”ì¼ ë“± ì‚¬ìš©)
        message = f"""
=== ì˜í™” ì¶”ì²œ ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ===
ëª¨ë¸: {model_name} v{model_version}
ìŠ¹ê²© ìƒíƒœ: {promotion_status}
ì„±ëŠ¥ ë©”íŠ¸ë¦­:
- RMSE: {evaluation_metrics['test_rmse']:.4f}
- MAE: {evaluation_metrics['test_mae']:.4f}
- í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {evaluation_metrics['test_samples']}
ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
        
        logger.info(message)
        
        # ì„±ëŠ¥ì´ ì¢‹ê³  í”„ë¡œë•ì…˜ì— ë°°í¬ëœ ê²½ìš° íŠ¹ë³„ ì•Œë¦¼
        if promotion_status == "production":
            logger.info("ğŸ‰ ìƒˆë¡œìš´ ëª¨ë¸ì´ í”„ë¡œë•ì…˜ì— ë°°í¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        return message
        
    except Exception as e:
        logger.error(f"ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {e}")
        pass


# íƒœìŠ¤í¬ ì •ì˜
setup_experiment_task = PythonOperator(
    task_id='setup_mlflow_experiment',
    python_callable=setup_mlflow_experiment,
    dag=dag,
)

train_model_task = PythonOperator(
    task_id='train_pytorch_model',
    python_callable=train_pytorch_model_with_mlflow,
    dag=dag,
)

register_model_task = PythonOperator(
    task_id='register_model',
    python_callable=register_model_to_registry,
    dag=dag,
)

evaluate_model_task = PythonOperator(
    task_id='evaluate_model',
    python_callable=evaluate_model_performance,
    dag=dag,
)

decide_promotion_task = PythonOperator(
    task_id='decide_promotion',
    python_callable=decide_model_promotion,
    dag=dag,
)

cleanup_models_task = PythonOperator(
    task_id='cleanup_old_models',
    python_callable=cleanup_old_models,
    dag=dag,
)

notification_task = PythonOperator(
    task_id='send_notification',
    python_callable=send_notification,
    dag=dag,
)

# ì¶”ê°€ ìœ í‹¸ë¦¬í‹° íƒœìŠ¤í¬ë“¤
check_data_quality_task = BashOperator(
    task_id='check_data_quality',
    bash_command='python -c "import pandas as pd; df=pd.read_csv(\\"data/processed/watch_log.csv\\"); print(f\\"Data quality check: {len(df)} rows, {df.isnull().sum().sum()} nulls\\")"',
    dag=dag,
)

# íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì •
setup_experiment_task >> check_data_quality_task >> train_model_task
train_model_task >> register_model_task >> evaluate_model_task
evaluate_model_task >> decide_promotion_task
decide_promotion_task >> [cleanup_models_task, notification_task]
